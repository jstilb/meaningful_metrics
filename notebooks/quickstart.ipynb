{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jstilb/meaningful_metrics/blob/main/notebooks/quickstart.ipynb)\n",
    "\n",
    "# Meaningful Metrics \u2014 Quick Start Notebook\n",
    "\n",
    "[![PyPI](https://img.shields.io/pypi/v/meaningful-metrics)](https://pypi.org/project/meaningful-metrics/)\n",
    "\n",
    "This notebook introduces the **Meaningful Metrics** framework \u2014 a Python library for evaluating AI products against human wellbeing rather than engagement.\n",
    "\n",
    "**In this notebook you'll learn to:**\n",
    "1. Install and import the library\n",
    "2. Define user goals and domain priorities\n",
    "3. Calculate core metrics: Quality Time Score, Goal Alignment, Distraction Ratio\n",
    "4. Generate a full metrics report with recommendations\n",
    "5. Apply the framework to evaluate an AI product\n",
    "\n",
    "---\n",
    "\n",
    "**Framework philosophy:** Most digital products optimize for engagement \u2014 session duration, clicks, DAUs. Meaningful Metrics replaces these proxies with direct measures of whether users' time advances their *declared* goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install the package (only needed once per runtime)\n",
    "!pip install meaningful-metrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "from meaningful_metrics import (\n",
    "    calculate_quality_time_score,\n",
    "    calculate_goal_alignment,\n",
    "    calculate_distraction_ratio,\n",
    "    calculate_actionability_score,\n",
    "    generate_metrics_report,\n",
    "    calculate_domain_contributions,\n",
    ")\n",
    "from meaningful_metrics.schemas import (\n",
    "    Goal,\n",
    "    DomainPriority,\n",
    "    TimeEntry,\n",
    "    ActionLog,\n",
    "    ActionWeights,\n",
    ")\n",
    "\n",
    "print(f\"meaningful_metrics imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## Part 1: Core Concepts\n",
    "\n",
    "The framework has three input types:\n",
    "\n",
    "| Type | Purpose | Example |\n",
    "|------|---------|------|\n",
    "| `Goal` | What the user wants to achieve | \"Learn Python\", linked to domains [\"coding\", \"tutorials\"] |\n",
    "| `DomainPriority` | How valuable each domain is (0\u20131) | coding=1.0, social_media=0.2 |\n",
    "| `TimeEntry` | Time actually spent in each domain | coding=3.0h, social_media=1.5h |\n",
    "\n",
    "And one output type: `MetricsReport` \u2014 which aggregates everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_goals"
   },
   "outputs": [],
   "source": [
    "# Step 1: Define what the user wants to achieve\n",
    "goals = [\n",
    "    Goal(\n",
    "        id=\"learn-python\",\n",
    "        name=\"Learn Python\",\n",
    "        domains=[\"coding\", \"tutorials\", \"documentation\"],\n",
    "        target_hours_per_week=8.0,\n",
    "    ),\n",
    "    Goal(\n",
    "        id=\"stay-healthy\",\n",
    "        name=\"Stay Healthy\",\n",
    "        domains=[\"exercise\", \"meal_prep\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Goals defined:\")\n",
    "for g in goals:\n",
    "    print(f\"  {g.name} \u2192 domains: {g.domains}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_priorities"
   },
   "outputs": [],
   "source": [
    "# Step 2: Set domain priorities (user-controlled, not algorithmic)\n",
    "# max_daily_hours applies diminishing returns \u2014 time beyond cap counts less\n",
    "priorities = [\n",
    "    DomainPriority(domain=\"coding\", priority=1.0, max_daily_hours=4.0),\n",
    "    DomainPriority(domain=\"tutorials\", priority=0.9, max_daily_hours=2.0),\n",
    "    DomainPriority(domain=\"documentation\", priority=0.7, max_daily_hours=1.0),\n",
    "    DomainPriority(domain=\"exercise\", priority=0.8, max_daily_hours=1.5),\n",
    "    DomainPriority(domain=\"meal_prep\", priority=0.5, max_daily_hours=1.0),\n",
    "    DomainPriority(domain=\"social_media\", priority=0.1),\n",
    "    DomainPriority(domain=\"news\", priority=0.3),\n",
    "]\n",
    "\n",
    "print(\"Domain priorities:\")\n",
    "for p in sorted(priorities, key=lambda x: x.priority, reverse=True):\n",
    "    cap_str = f\" (cap: {p.max_daily_hours}h)\" if p.max_daily_hours else \"\"\n",
    "    print(f\"  {p.domain:<20} priority={p.priority:.1f}{cap_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_entries"
   },
   "outputs": [],
   "source": [
    "# Step 3: Log today's time\n",
    "entries = [\n",
    "    TimeEntry(domain=\"coding\", hours=3.0),\n",
    "    TimeEntry(domain=\"tutorials\", hours=1.5),\n",
    "    TimeEntry(domain=\"social_media\", hours=2.0),\n",
    "    TimeEntry(domain=\"exercise\", hours=0.75),\n",
    "    TimeEntry(domain=\"news\", hours=0.5),\n",
    "    TimeEntry(domain=\"meal_prep\", hours=0.5),\n",
    "]\n",
    "\n",
    "total_hours = sum(e.hours for e in entries)\n",
    "print(f\"Total hours tracked: {total_hours:.1f}h\")\n",
    "print(\"\\nTime log:\")\n",
    "for e in sorted(entries, key=lambda x: x.hours, reverse=True):\n",
    "    bar = \"\u2588\" * int(e.hours * 4)\n",
    "    print(f\"  {e.domain:<20} {bar:<20} {e.hours:.1f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## Part 2: Individual Metrics\n",
    "\n",
    "You can call each metric function directly for targeted calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qts"
   },
   "outputs": [],
   "source": [
    "# Quality Time Score: priority-weighted time with diminishing returns\n",
    "qts = calculate_quality_time_score(entries, priorities)\n",
    "print(f\"Quality Time Score: {qts:.2f}\")\n",
    "print()\n",
    "\n",
    "# Domain-level breakdown\n",
    "domain_metrics = calculate_domain_contributions(entries, priorities)\n",
    "print(\"QTS breakdown by domain:\")\n",
    "for dm in sorted(domain_metrics, key=lambda x: x.contribution, reverse=True):\n",
    "    capped = \" (capped)\" if dm.effective_time < dm.time_spent else \"\"\n",
    "    print(\n",
    "        f\"  {dm.domain:<20} \"\n",
    "        f\"{dm.time_spent:.1f}h \u00d7 {dm.priority:.1f} priority\"\n",
    "        f\"{capped} = {dm.contribution:.2f} QTS\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goal_alignment"
   },
   "outputs": [],
   "source": [
    "# Goal Alignment: what % of time advances stated goals?\n",
    "alignment = calculate_goal_alignment(entries, goals)\n",
    "distraction = calculate_distraction_ratio(entries, goals)\n",
    "\n",
    "print(f\"Goal Alignment:   {alignment:.1f}%\")\n",
    "print(f\"Distraction Ratio: {distraction:.1f}%\")\n",
    "\n",
    "# Visualize\n",
    "bar_len = 40\n",
    "aligned_bars = int((alignment / 100) * bar_len)\n",
    "distracted_bars = bar_len - aligned_bars\n",
    "print(f\"\\n[{'\u2588' * aligned_bars}{'\u2591' * distracted_bars}]\")\n",
    "print(f\" {'Goal-aligned':>{aligned_bars - 1}} | {'Distracted':<{distracted_bars}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "actionability"
   },
   "outputs": [],
   "source": [
    "# Actionability Score: does consumed content translate into action?\n",
    "# Imagine tracking your AI assistant interactions today:\n",
    "# - Consumed 25 responses\n",
    "# - Bookmarked 5 (saved to notes)\n",
    "# - Shared 2 (sent to colleague)\n",
    "# - Applied 8 (directly used in a deliverable)\n",
    "\n",
    "actionability = calculate_actionability_score(\n",
    "    consumed=25,\n",
    "    bookmarked=5,\n",
    "    shared=2,\n",
    "    applied=8,\n",
    ")\n",
    "\n",
    "# Custom weights (optional)\n",
    "custom_weights = ActionWeights(bookmarked=0.3, shared=0.5, applied=1.0)\n",
    "\n",
    "print(f\"Actionability Score: {actionability:.3f}\")\n",
    "print(f\"(Default weights: bookmarked=0.3, shared=0.5, applied=1.0)\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "if actionability > 0.5:\n",
    "    label = \"HIGH \u2014 you're acting on what you consume\"\n",
    "elif actionability > 0.2:\n",
    "    label = \"MODERATE \u2014 some output, room to improve\"\n",
    "else:\n",
    "    label = \"LOW \u2014 mostly passive consumption\"\n",
    "print(f\"Rating: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## Part 3: Full Metrics Report\n",
    "\n",
    "`generate_metrics_report` combines all metrics into a structured report with recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_report"
   },
   "outputs": [],
   "source": [
    "action_log = ActionLog(consumed=25, bookmarked=5, shared=2, applied=8)\n",
    "\n",
    "report = generate_metrics_report(\n",
    "    time_entries=entries,\n",
    "    priorities=priorities,\n",
    "    goals=goals,\n",
    "    actions=action_log,\n",
    "    period=\"daily\",\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DAILY METRICS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Quality Time Score:   {report.quality_time_score:.2f}\")\n",
    "print(f\"Raw Time Tracked:     {report.raw_time_hours:.1f}h\")\n",
    "print(f\"Goal Alignment:       {report.goal_alignment_percent:.1f}%\")\n",
    "print(f\"Distraction Ratio:    {report.distraction_percent:.1f}%\")\n",
    "print(f\"Actionability Score:  {report.actionability_score:.3f}\")\n",
    "print()\n",
    "print(\"Recommendations:\")\n",
    "for rec in report.recommendations:\n",
    "    icons = {\"high\": \"[!!!]\", \"medium\": \"[!! ]\", \"low\": \"[ i ]\"}\n",
    "    print(f\"  {icons[rec.priority]} {rec.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## Part 4: AI Product Evaluation\n",
    "\n",
    "The real power of Meaningful Metrics is evaluating AI products. Here's a mini version of the ChatGPT case study \u2014 modeling how users' time with an AI assistant maps to their goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai_eval"
   },
   "outputs": [],
   "source": [
    "# Evaluating an AI writing assistant\n",
    "# User's declared goal: improve their writing craft\n",
    "\n",
    "writing_goals = [\n",
    "    Goal(\n",
    "        id=\"improve-craft\",\n",
    "        name=\"Improve Writing Craft\",\n",
    "        domains=[\"drafting_with_feedback\", \"style_learning\", \"revision_practice\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "writing_priorities = [\n",
    "    DomainPriority(domain=\"drafting_with_feedback\", priority=1.0, max_daily_hours=1.5),\n",
    "    DomainPriority(domain=\"revision_practice\", priority=0.9, max_daily_hours=1.0),\n",
    "    DomainPriority(domain=\"style_learning\", priority=0.8, max_daily_hours=0.5),\n",
    "    DomainPriority(domain=\"ai_ghostwriting\", priority=0.1),  # AI writes for user\n",
    "    DomainPriority(domain=\"off_task\", priority=0.05),\n",
    "]\n",
    "\n",
    "# Scenario A: The user is using AI as a tutor (good pattern)\n",
    "tutor_entries = [\n",
    "    TimeEntry(domain=\"drafting_with_feedback\", hours=1.0),\n",
    "    TimeEntry(domain=\"revision_practice\", hours=0.5),\n",
    "    TimeEntry(domain=\"style_learning\", hours=0.3),\n",
    "    TimeEntry(domain=\"off_task\", hours=0.2),\n",
    "]\n",
    "\n",
    "# Scenario B: The user is using AI to write for them (engagement without growth)\n",
    "ghostwriter_entries = [\n",
    "    TimeEntry(domain=\"ai_ghostwriting\", hours=1.5),\n",
    "    TimeEntry(domain=\"off_task\", hours=0.5),\n",
    "    TimeEntry(domain=\"drafting_with_feedback\", hours=0.0),\n",
    "    TimeEntry(domain=\"revision_practice\", hours=0.0),\n",
    "]\n",
    "\n",
    "report_a = generate_metrics_report(tutor_entries, writing_priorities, writing_goals)\n",
    "report_b = generate_metrics_report(ghostwriter_entries, writing_priorities, writing_goals)\n",
    "\n",
    "print(\"SCENARIO A: AI as Tutor\")\n",
    "print(f\"  Goal Alignment: {report_a.goal_alignment_percent:.1f}%\")\n",
    "print(f\"  Quality Time Score: {report_a.quality_time_score:.2f}\")\n",
    "print()\n",
    "print(\"SCENARIO B: AI as Ghostwriter\")\n",
    "print(f\"  Goal Alignment: {report_b.goal_alignment_percent:.1f}%\")\n",
    "print(f\"  Quality Time Score: {report_b.quality_time_score:.2f}\")\n",
    "print()\n",
    "print(\"KEY INSIGHT: Both scenarios generate similar engagement metrics\")\n",
    "print(\"(time with the product, messages sent, tokens consumed).\")\n",
    "print(\"Meaningful Metrics reveals that only Scenario A advances the user's goal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- Read the full [ChatGPT Case Study](https://github.com/jstilb/meaningful_metrics/blob/main/results/case-studies/chatgpt-goal-alignment.md)\n",
    "- Browse the [API Reference](https://jstilb.github.io/meaningful_metrics/api/)\n",
    "- Contribute your own metrics or case studies via [GitHub](https://github.com/jstilb/meaningful_metrics)\n",
    "\n",
    "---\n",
    "\n",
    "*Meaningful Metrics is open source under the MIT License.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Meaningful Metrics Quick Start",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}