# <Evaluation Name>

## Summary
Briefly describe the evaluation's purpose and the ethical or human-centered objective it supports.

## When to Use This Evaluation
Explain the product scenarios, model types, or deployment contexts where this evaluation adds the most value.

## Evaluation Objectives
List the core questions the evaluation should answer about model behavior, stakeholder impact, and responsible use.

## Test Assets & Signals
- **Input data:** What datasets, prompts, or scenarios are required? Note consent, provenance, and refresh cadence.
- **Metrics & rubrics:** Define quantitative scores, qualitative rubrics, or combined indices that reveal success vs. failure.
- **Guardrails:** Document thresholds, sensitive cases, or disallowed outcomes that trigger remediation.

## Execution Playbook
1. Outline the step-by-step workflow for assembling evaluators, running automated checks, and capturing evidence.
2. Highlight any tooling or scripts teams should prepare.
3. Specify how human feedback is recorded and triaged.

## Acceptance Criteria & Reporting
Describe how to interpret results, decision gates for launch or rollback, and how to communicate findings to stakeholders.

## Governance & Maintenance
Cover responsible owners, review cadence, incident response triggers, and how this evaluation evolves with new capabilities.

## References & Inspiration
Cite research, regulations, or industry resources that guided the evaluation design.
