# <Evaluation Name>

## Summary
Briefly describe the evaluation's purpose and the ethical or human-centered objective it supports.

## Maturity Tier
Select **Idea**, **Draft**, or **Field-Tested**. Provide evidence (pilots, production runs, audit history) supporting this tier.

## When to Use This Evaluation
Explain the product scenarios, model types, or deployment contexts where this evaluation adds the most value.

## Evaluation Objectives
List the core questions the evaluation should answer about model behavior, stakeholder impact, and responsible use.

## Test Assets & Signals
- **Input data:** What datasets, prompts, or scenarios are required? Note consent, provenance, and refresh cadence.
- **Metrics & rubrics:** Define quantitative scores, qualitative rubrics, or combined indices that reveal success vs. failure.
- **Guardrails:** Document thresholds, sensitive cases, or disallowed outcomes that trigger remediation.
- **Tooling links:** Reference scripts or checklists in `tooling/evals/<evaluation_name>/`.

## Execution Playbook
1. Outline the step-by-step workflow for assembling evaluators, running automated checks, and capturing evidence.
2. Highlight any tooling or scripts teams should prepare.
3. Specify how human feedback is recorded and triaged.

## Acceptance Criteria & Reporting
Describe how to interpret results, decision gates for launch or rollback, and how to communicate findings to stakeholders.

## Governance & Maintenance
Cover responsible owners, review cadence, incident response triggers, and how this evaluation evolves with new capabilities. Include how you will use toolkit assets (decision logs, fairness audits, surveys) and document updates in the community changelog.

## References & Inspiration
Cite research, regulations, or industry resources that guided the evaluation design.
