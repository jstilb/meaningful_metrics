{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Meaningful Metrics","text":"<p>A Python framework for evaluating AI systems against human wellbeing rather than engagement.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Modern AI products are optimized for a single goal: maximize engagement. Watch time, click-through rates, session duration \u2014 these metrics reward the algorithm for keeping users on the platform, regardless of whether that time was spent well.</p> <p>The result: recommendation systems that pull users toward outrage, social platforms designed to trigger compulsive checking, and AI assistants that flatter rather than inform.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>Meaningful Metrics provides a composable set of evaluation primitives that measure what actually matters:</p> <ul> <li>Goal Alignment \u2014 Is the user making progress on what they said they wanted to do?</li> <li>Quality Time Score \u2014 Is time being spent on high-priority, purposeful activities?</li> <li>Actionability Score \u2014 Does consumed information translate into action?</li> <li>Distraction Ratio \u2014 How much time is lost to low-priority activities?</li> <li>Locality Score \u2014 Is content relevant to the user's actual context?</li> </ul> <p>These metrics are differentiable (suitable for ML optimization), transparent (documented formulas), and user-controlled (user sets the priorities).</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from meaningful_metrics import (\n    calculate_quality_time_score,\n    calculate_goal_alignment,\n    generate_metrics_report,\n)\nfrom meaningful_metrics.schemas import (\n    TimeEntry, DomainPriority, Goal\n)\n\n# User defines their goals\ngoals = [\n    Goal(id=\"learn\", name=\"Learn Python\", domains=[\"coding\", \"tutorials\"]),\n    Goal(id=\"fitness\", name=\"Stay Fit\", domains=[\"exercise\"]),\n]\n\n# User defines domain priorities\npriorities = [\n    DomainPriority(domain=\"coding\", priority=1.0, max_daily_hours=4.0),\n    DomainPriority(domain=\"tutorials\", priority=0.9, max_daily_hours=2.0),\n    DomainPriority(domain=\"exercise\", priority=0.8, max_daily_hours=1.5),\n    DomainPriority(domain=\"social_media\", priority=0.2),\n]\n\n# Today's time entries\nentries = [\n    TimeEntry(domain=\"coding\", hours=3.0),\n    TimeEntry(domain=\"social_media\", hours=1.5),\n    TimeEntry(domain=\"exercise\", hours=0.5),\n]\n\n# Generate report\nreport = generate_metrics_report(entries, priorities, goals)\n\nprint(f\"Quality Time Score: {report.quality_time_score:.2f}\")\nprint(f\"Goal Alignment: {report.goal_alignment_percent:.1f}%\")\nprint(f\"Distraction: {report.distraction_percent:.1f}%\")\n\n# Quality Time Score: 3.30\n# Goal Alignment: 70.0%\n# Distraction: 30.0%\n</code></pre>"},{"location":"#connection-to-constitutional-ai","title":"Connection to Constitutional AI","text":"<p>This framework is philosophically aligned with Anthropic's Constitutional AI approach to AI evaluation. Where Constitutional AI asks models to evaluate outputs against a set of human values, Meaningful Metrics asks product teams to evaluate user experiences against declared human intentions.</p> <p>Both approaches share a core conviction: AI systems should be accountable to explicit human principles, not implicit engagement signals.</p> <p>The Meaningful Metrics framework operationalizes this at the product layer \u2014 giving teams concrete measurements to hold their AI features accountable to user wellbeing.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install meaningful-metrics\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2014 Install and run your first evaluation in 5 minutes</li> <li>Concepts \u2014 Understand the framework's core ideas</li> <li>API Reference \u2014 Full function documentation</li> <li>Case Studies \u2014 Real-world evaluations</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Meaningful Metrics is a Python library that provides a composable framework for evaluating AI systems against human-centered outcomes rather than engagement-maximization proxies. The architecture prioritizes transparency, extensibility, and mathematical rigor.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph Input Layer\n        TE[TimeEntry] --&gt; MC[Metrics Core]\n        DP[DomainPriority] --&gt; MC\n        G[Goal] --&gt; MC\n        AL[ActionLog] --&gt; MC\n    end\n\n    subgraph Core Engine\n        MC --&gt; QTS[Quality Time Score]\n        MC --&gt; GA[Goal Alignment]\n        MC --&gt; DR[Distraction Ratio]\n        MC --&gt; AS[Actionability Score]\n        MC --&gt; LS[Locality Score]\n    end\n\n    subgraph Scoring Layer\n        QTS --&gt; DC[Domain Contributions]\n        GA --&gt; REC[Recommendations Engine]\n        DR --&gt; REC\n        DC --&gt; MR[Metrics Report]\n        REC --&gt; MR\n        AS --&gt; MR\n    end\n\n    subgraph Output Layer\n        MR --&gt; JSON[JSON Export]\n        MR --&gt; VIZ[Visualizations]\n        MR --&gt; DASH[Dashboards]\n    end</code></pre>"},{"location":"architecture/#module-dependency-graph","title":"Module Dependency Graph","text":"<pre><code>graph LR\n    subgraph meaningful_metrics\n        schemas[schemas.py] --&gt; metrics[metrics.py]\n        schemas --&gt; scoring[scoring.py]\n        metrics --&gt; scoring\n        schemas --&gt; init[__init__.py]\n        metrics --&gt; init\n        scoring --&gt; init\n    end</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Schemas\n    participant Metrics\n    participant Scoring\n    participant Report\n\n    User-&gt;&gt;Schemas: Define goals, priorities, time entries\n    Schemas-&gt;&gt;Schemas: Pydantic validation\n    User-&gt;&gt;Scoring: generate_metrics_report()\n    Scoring-&gt;&gt;Metrics: calculate_quality_time_score()\n    Scoring-&gt;&gt;Metrics: calculate_goal_alignment()\n    Scoring-&gt;&gt;Metrics: calculate_distraction_ratio()\n    Scoring-&gt;&gt;Metrics: calculate_actionability_score()\n    Scoring-&gt;&gt;Scoring: calculate_domain_contributions()\n    Scoring-&gt;&gt;Scoring: generate_recommendations()\n    Scoring-&gt;&gt;Report: MetricsReport\n    Report-&gt;&gt;User: Structured report with recommendations</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-schema-layer-schemaspy","title":"1. Schema Layer (<code>schemas.py</code>)","text":"<p>Pydantic models that enforce data contracts at the boundary. All input validation happens here, ensuring that downstream metric calculations receive well-formed data.</p> <p>Key design decisions: - Pydantic v2 for runtime validation with type safety - Strict bounds on numeric fields (priorities 0-1, hours &gt;= 0) - Optional fields with sensible defaults (e.g., <code>ActionWeights</code>)</p>"},{"location":"architecture/#2-metrics-core-metricspy","title":"2. Metrics Core (<code>metrics.py</code>)","text":"<p>Pure functions that implement the mathematical formulas. Each metric is:</p> <ul> <li>Stateless: No side effects, deterministic output</li> <li>Differentiable: Smooth approximations available for ML optimization (e.g., <code>soft_min</code>)</li> <li>Documented: Formula, parameters, and edge cases described in docstrings</li> </ul> Metric Formula Range Quality Time Score <code>sum(min(Ti, Capi) * Pi)</code> <code>[0, inf)</code> Goal Alignment <code>(goal_time / total_time) * 100</code> <code>[0, 100]</code> Distraction Ratio <code>100 - Goal Alignment</code> <code>[0, 100]</code> Actionability Score <code>weighted_actions / consumed</code> <code>[0, inf)</code> Locality Score <code>relevance * engagement</code> <code>[0, 1]</code>"},{"location":"architecture/#3-scoring-layer-scoringpy","title":"3. Scoring Layer (<code>scoring.py</code>)","text":"<p>Composite functions that aggregate individual metrics into actionable reports. The recommendation engine applies heuristic rules against computed metrics to surface improvement opportunities.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ol> <li>Composition over inheritance -- Metrics are standalone functions composed in <code>generate_metrics_report()</code>.</li> <li>Validation at the boundary -- Pydantic models validate all input; core functions trust their inputs.</li> <li>Transparency -- Every formula is documented with LaTeX-style notation in docstrings.</li> <li>Extensibility -- New metrics follow the same pattern: define schema, implement function, compose in report.</li> <li>ML-friendliness -- Smooth approximations (<code>soft_min</code>) enable gradient-based optimization.</li> </ol>"},{"location":"concepts/","title":"Framework Concepts","text":"<p>Meaningful Metrics is built on a set of composable, transparent primitives. This page explains the \"why\" behind the framework's design.</p>"},{"location":"concepts/#the-engagement-trap","title":"The Engagement Trap","text":"<p>Traditional digital product metrics are defined by what is easy to measure, not what matters:</p> Metric What It Optimizes What It Ignores Session duration More time on platform Whether time was well-spent Click-through rate More clicks Whether action was desired Daily active users More frequent visits Whether visits were valuable Watch time More content consumed Whether content was beneficial <p>These metrics are not malicious \u2014 they're just easy to collect. But optimizing for them creates products that extract attention rather than respect it.</p>"},{"location":"concepts/#human-centered-alternatives","title":"Human-Centered Alternatives","text":"<p>Meaningful Metrics replaces these proxies with direct measures of human outcomes:</p>"},{"location":"concepts/#1-quality-time-score-qts","title":"1. Quality Time Score (QTS)","text":"<p>The question it answers: How much valuable time did the user actually spend?</p> <p>QTS weights time by priority and applies diminishing returns caps to prevent over-optimization on a single domain.</p> <pre><code>QTS = sum(min(Ti, Capi) * Pi)\n</code></pre> <p>Where <code>Ti</code> is time spent, <code>Capi</code> is the diminishing returns cap, and <code>Pi</code> is the user's priority weight.</p> <p>See Quality Time Score for full details.</p>"},{"location":"concepts/#2-goal-alignment","title":"2. Goal Alignment","text":"<p>The question it answers: Is the user making progress on what they said they care about?</p> <pre><code>Goal Alignment = (sum(Time on goal domains) / Total time) * 100%\n</code></pre> <p>See Goal Alignment for full details.</p>"},{"location":"concepts/#3-actionability-score","title":"3. Actionability Score","text":"<p>The question it answers: Does consumed information translate into meaningful action?</p> <pre><code>AS = (bookmarked * 0.3 + shared * 0.5 + applied * 1.0) / consumed\n</code></pre> <p>See Actionability for full details.</p>"},{"location":"concepts/#4-distraction-ratio","title":"4. Distraction Ratio","text":"<p>The question it answers: How much time is being lost to low-priority activities?</p> <pre><code>Distraction Ratio = 100% - Goal Alignment\n</code></pre>"},{"location":"concepts/#5-locality-score","title":"5. Locality Score","text":"<p>The question it answers: Is content relevant to the user's actual community and context?</p> <pre><code>Locality Score = local_relevance * engagement\n</code></pre>"},{"location":"concepts/#design-principles","title":"Design Principles","text":""},{"location":"concepts/#user-controlled-parameters","title":"User-Controlled Parameters","text":"<p>Every metric in this framework is parameterized by user-defined values:</p> <ul> <li>Priorities are set by the user, not inferred by the platform</li> <li>Goals are declared by the user, not predicted by the algorithm</li> <li>Caps are defined by the user, not optimized by engagement loops</li> </ul> <p>This is a deliberate inversion of the standard model. Most platforms use behavioral data to infer what users \"want\" \u2014 then optimize for that inferred preference. Meaningful Metrics starts with explicit user declarations.</p>"},{"location":"concepts/#transparent-formulas","title":"Transparent Formulas","text":"<p>Every metric has a documented formula. There are no black-box scoring systems. Users and developers can inspect, audit, and challenge any calculation.</p>"},{"location":"concepts/#differentiable-for-ml","title":"Differentiable (for ML)","text":"<p>All metrics are designed to be usable as training signals for ML systems. The <code>soft_min</code> function provides a differentiable approximation of the diminishing returns cap, enabling gradient-based optimization.</p>"},{"location":"concepts/#privacy-preserving","title":"Privacy-Preserving","text":"<p>The framework processes time entries and action logs. It does not require user identity, behavioral fingerprints, or third-party tracking.</p>"},{"location":"concepts/#relationship-to-constitutional-ai","title":"Relationship to Constitutional AI","text":"<p>Anthropic's Constitutional AI approach establishes explicit principles that AI systems are evaluated against during training. Meaningful Metrics applies the same logic at the product evaluation layer.</p> <p>Where Constitutional AI asks: \"Does this model output violate human values?\"</p> <p>Meaningful Metrics asks: \"Does this product experience violate the user's declared intentions?\"</p> <p>Both frameworks share the conviction that AI systems need explicit human principles as guardrails \u2014 not just implicit feedback from engagement data.</p>"},{"location":"concepts/#when-to-use-this-framework","title":"When to Use This Framework","text":"<p>Good fit:</p> <ul> <li>Evaluating AI assistant products (chatbots, recommendation systems)</li> <li>Designing training incentives for RLHF pipelines</li> <li>Auditing existing products for alignment with user wellbeing</li> <li>Research on human-AI interaction quality</li> </ul> <p>Not designed for:</p> <ul> <li>Real-time content ranking (too computationally expensive per-request)</li> <li>Individual recommendation scoring</li> <li>Aggregate population-level analytics without individual goal data</li> </ul>"},{"location":"eval_template/","title":"","text":""},{"location":"eval_template/#summary","title":"Summary","text":"<p>Briefly describe the evaluation's purpose and the ethical or human-centered objective it supports.</p>"},{"location":"eval_template/#maturity-tier","title":"Maturity Tier","text":"<p>Select Idea, Draft, or Field-Tested. Provide evidence (pilots, production runs, audit history) supporting this tier.</p>"},{"location":"eval_template/#when-to-use-this-evaluation","title":"When to Use This Evaluation","text":"<p>Explain the product scenarios, model types, or deployment contexts where this evaluation adds the most value.</p>"},{"location":"eval_template/#evaluation-objectives","title":"Evaluation Objectives","text":"<p>List the core questions the evaluation should answer about model behavior, stakeholder impact, and responsible use.</p>"},{"location":"eval_template/#test-assets-signals","title":"Test Assets &amp; Signals","text":"<ul> <li>Input data: What datasets, prompts, or scenarios are required? Note consent, provenance, and refresh cadence.</li> <li>Metrics &amp; rubrics: Define quantitative scores, qualitative rubrics, or combined indices that reveal success vs. failure.</li> <li>Guardrails: Document thresholds, sensitive cases, or disallowed outcomes that trigger remediation.</li> <li>Tooling links: Reference scripts or checklists in <code>tooling/evals/&lt;evaluation_name&gt;/</code>.</li> </ul>"},{"location":"eval_template/#execution-playbook","title":"Execution Playbook","text":"<ol> <li>Outline the step-by-step workflow for assembling evaluators, running automated checks, and capturing evidence.</li> <li>Highlight any tooling or scripts teams should prepare.</li> <li>Specify how human feedback is recorded and triaged.</li> </ol>"},{"location":"eval_template/#acceptance-criteria-reporting","title":"Acceptance Criteria &amp; Reporting","text":"<p>Describe how to interpret results, decision gates for launch or rollback, and how to communicate findings to stakeholders.</p>"},{"location":"eval_template/#governance-maintenance","title":"Governance &amp; Maintenance","text":"<p>Cover responsible owners, review cadence, incident response triggers, and how this evaluation evolves with new capabilities. Include how you will use toolkit assets (decision logs, fairness audits, surveys) and document updates in the community changelog.</p>"},{"location":"eval_template/#references-inspiration","title":"References &amp; Inspiration","text":"<p>Cite research, regulations, or industry resources that guided the evaluation design.</p>"},{"location":"metric_template/","title":"Metric Proposal Template","text":"<p>Use this template to document new metrics for the Meaningful Metrics repository. Copy the sections into a new Markdown file inside <code>metrics/</code> (or a relevant subdirectory) and replace the guidance text with your content.</p>"},{"location":"metric_template/#metric-name","title":"Metric Name","text":"<p>Provide a concise, action-oriented name (e.g., \"Engaged Learning Time\").</p>"},{"location":"metric_template/#summary","title":"Summary","text":"<p>Describe the human-centered outcome this metric promotes in 2\u20133 sentences. Focus on the benefit to people rather than the product.</p>"},{"location":"metric_template/#maturity-tier","title":"Maturity Tier","text":"<p>Select Idea, Draft, or Field-Tested. Summarize the evidence that supports this tier (pilots, production data, community validation).</p>"},{"location":"metric_template/#context-motivation","title":"Context &amp; Motivation","text":"<ul> <li>What problem does this metric address?</li> <li>Who is affected, and what do they need?</li> <li>Why are existing or traditional metrics insufficient?</li> </ul>"},{"location":"metric_template/#desired-behaviors","title":"Desired Behaviors","text":"<p>List the positive behaviors or outcomes this metric should increase. Mention any harmful behaviors it should decrease or disincentivize.</p>"},{"location":"metric_template/#measurement-strategy","title":"Measurement Strategy","text":"<ul> <li>Signals: What data sources or events capture the desired behavior?</li> <li>Calculation: Provide a clear formula or algorithm.</li> <li>Tapering or Guardrails: Explain how the metric avoids over-optimization or binge patterns.</li> <li>Frequency: How often should it be measured and reviewed?</li> </ul>"},{"location":"metric_template/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Data quality or privacy requirements</li> <li>Instrumentation needs (reference assets in <code>tooling/metrics/&lt;metric_name&gt;/</code> when available)</li> <li>Potential biases or failure modes and how to mitigate them</li> </ul>"},{"location":"metric_template/#validation-feedback","title":"Validation &amp; Feedback","text":"<ul> <li>How will you evaluate whether the metric reflects the intended outcome?</li> <li>What qualitative or quantitative feedback loops are needed?</li> <li>Which governance assets (decision log, fairness audit, surveys) will you use from <code>docs/toolkit/</code>?</li> </ul>"},{"location":"metric_template/#ethical-checklist","title":"Ethical Checklist","text":"<ul> <li>Does this metric respect user agency and autonomy?</li> <li>Are there groups who might be disproportionately impacted?</li> <li>How will harms be detected and addressed?</li> </ul>"},{"location":"metric_template/#references-inspiration","title":"References &amp; Inspiration","text":"<p>Cite research, blog posts, or case studies that informed this metric.</p>"},{"location":"model_card/","title":"Model Card: Meaningful Metrics Scoring Framework","text":""},{"location":"model_card/#model-details","title":"Model Details","text":"<ul> <li>Name: Meaningful Metrics v0.1.0</li> <li>Type: Rule-based scoring framework (not a trained ML model)</li> <li>Authors: jstilb</li> <li>License: MIT</li> <li>Repository: https://github.com/jstilb/meaningful_metrics</li> </ul>"},{"location":"model_card/#intended-use","title":"Intended Use","text":""},{"location":"model_card/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Product teams evaluating whether their AI systems optimize for user wellbeing over engagement maximization</li> <li>Researchers studying the impact of attention-economy metrics on user behavior</li> <li>Policy analysts assessing platform accountability through human-centered KPIs</li> <li>ML engineers incorporating wellbeing-aligned reward signals into training pipelines</li> </ul>"},{"location":"model_card/#out-of-scope","title":"Out of Scope","text":"<ul> <li>Real-time production inference (designed for batch evaluation)</li> <li>Replacing domain-specific clinical or psychological assessments</li> <li>Automated decision-making without human review</li> </ul>"},{"location":"model_card/#metrics","title":"Metrics","text":"Metric Description Range Interpretation Quality Time Score Priority-weighted time with diminishing returns [0, inf) Higher = more time on high-priority activities Goal Alignment Percentage of time on stated goals [0, 100] Higher = more goal-directed behavior Distraction Ratio Percentage of time on non-goal activities [0, 100] Lower = less distraction Actionability Score Information-to-action conversion rate [0, inf) Higher = content drives concrete action Locality Score Community relevance weighting [0, 1] Higher = more locally relevant engagement"},{"location":"model_card/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"model_card/#potential-benefits","title":"Potential Benefits","text":"<ul> <li>Shifts optimization targets from engagement to wellbeing</li> <li>Makes diminishing returns explicit through configurable caps</li> <li>Provides transparency through documented formulas</li> <li>Supports user agency through user-defined goals and priorities</li> </ul>"},{"location":"model_card/#potential-risks","title":"Potential Risks","text":"<ul> <li>Gamification: Metrics could be gamed if users optimize for scores rather than genuine wellbeing</li> <li>Cultural bias: Default weight values reflect assumptions about \"productive\" vs. \"unproductive\" time that may not generalize across cultures</li> <li>Reductionism: Complex human experiences are simplified into numeric scores</li> <li>Privacy: Time-tracking data required for computation is inherently sensitive</li> </ul>"},{"location":"model_card/#mitigations","title":"Mitigations","text":"<ul> <li>All weights and caps are user-configurable -- no hardcoded value judgments</li> <li>Recommendations are advisory, not prescriptive</li> <li>The framework is designed for self-assessment, not external evaluation</li> <li>No data is collected or transmitted by the library itself</li> </ul>"},{"location":"model_card/#limitations","title":"Limitations","text":"<ul> <li>Formulas are heuristic, not empirically validated against wellbeing outcomes</li> <li>Diminishing returns use a linear cap rather than a smooth decay curve</li> <li>The recommendation engine uses fixed thresholds (e.g., 30% goal alignment triggers a warning)</li> <li>No temporal modeling -- each evaluation is a snapshot, not a trend</li> </ul>"},{"location":"model_card/#evaluation","title":"Evaluation","text":"<p>See <code>results/metrics.json</code> for benchmark evaluation results and <code>results/figures/</code> for visualizations of metric behavior under various input conditions.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with Meaningful Metrics in under 5 minutes.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install meaningful-metrics\n</code></pre> <p>Requires Python 3.11+.</p>"},{"location":"quickstart/#core-concepts-30-second-version","title":"Core Concepts (30-second version)","text":"<p>Meaningful Metrics has three inputs and one output:</p> Input What it is <code>TimeEntry</code> Time spent in a domain (e.g., \"coding: 2 hours\") <code>DomainPriority</code> How valuable that domain is to the user (0.0\u20131.0) <code>Goal</code> What the user is trying to achieve, linked to domains Output What it is <code>MetricsReport</code> QTS, goal alignment %, distraction %, recommendations"},{"location":"quickstart/#step-1-define-your-goals","title":"Step 1: Define Your Goals","text":"<p>Goals represent what the user wants to achieve. Each goal maps to one or more content domains.</p> <pre><code>from meaningful_metrics.schemas import Goal\n\ngoals = [\n    Goal(\n        id=\"learn-ml\",\n        name=\"Learn Machine Learning\",\n        domains=[\"ml_courses\", \"coding\", \"research_papers\"],\n        target_hours_per_week=10.0,\n    ),\n    Goal(\n        id=\"health\",\n        name=\"Maintain Health\",\n        domains=[\"exercise\", \"meal_prep\", \"sleep_tracking\"],\n    ),\n]\n</code></pre>"},{"location":"quickstart/#step-2-set-domain-priorities","title":"Step 2: Set Domain Priorities","text":"<p>Priorities weight how valuable each domain is. Set <code>max_daily_hours</code> to apply diminishing returns.</p> <pre><code>from meaningful_metrics.schemas import DomainPriority\n\npriorities = [\n    DomainPriority(domain=\"ml_courses\", priority=1.0, max_daily_hours=3.0),\n    DomainPriority(domain=\"coding\", priority=0.9, max_daily_hours=4.0),\n    DomainPriority(domain=\"research_papers\", priority=0.8),\n    DomainPriority(domain=\"exercise\", priority=0.7, max_daily_hours=1.5),\n    DomainPriority(domain=\"social_media\", priority=0.1),\n    DomainPriority(domain=\"news\", priority=0.3),\n]\n</code></pre>"},{"location":"quickstart/#step-3-log-time-entries","title":"Step 3: Log Time Entries","text":"<pre><code>from meaningful_metrics.schemas import TimeEntry\n\n# Today's time log\nentries = [\n    TimeEntry(domain=\"coding\", hours=2.5),\n    TimeEntry(domain=\"ml_courses\", hours=1.0),\n    TimeEntry(domain=\"social_media\", hours=2.0),\n    TimeEntry(domain=\"exercise\", hours=0.75),\n    TimeEntry(domain=\"news\", hours=0.5),\n]\n</code></pre>"},{"location":"quickstart/#step-4-generate-report","title":"Step 4: Generate Report","text":"<pre><code>from meaningful_metrics import generate_metrics_report\n\nreport = generate_metrics_report(\n    time_entries=entries,\n    priorities=priorities,\n    goals=goals,\n    period=\"daily\",\n)\n\nprint(f\"Quality Time Score: {report.quality_time_score:.2f}\")\nprint(f\"Goal Alignment:     {report.goal_alignment_percent:.1f}%\")\nprint(f\"Distraction Ratio:  {report.distraction_percent:.1f}%\")\nprint()\nprint(\"Recommendations:\")\nfor rec in report.recommendations:\n    print(f\"  [{rec.priority.upper()}] {rec.message}\")\n</code></pre> <p>Output:</p> <pre><code>Quality Time Score: 3.58\nGoal Alignment:     57.9%\nDistraction Ratio:  42.1%\n\nRecommendations:\n  [LOW] Great job! Goal alignment is 58%. Keep up the good work.\n  [MEDIUM] Spent 2.0h on low-priority domain 'social_media'. Consider reducing this time.\n</code></pre>"},{"location":"quickstart/#step-5-inspect-domain-breakdown","title":"Step 5: Inspect Domain Breakdown","text":"<pre><code>print(\"\\nBy Domain:\")\nfor domain in report.by_domain:\n    print(\n        f\"  {domain.domain}: \"\n        f\"{domain.time_spent:.1f}h spent, \"\n        f\"{domain.effective_time:.1f}h effective, \"\n        f\"contribution={domain.contribution:.2f}\"\n    )\n</code></pre> <p>Output:</p> <pre><code>By Domain:\n  coding: 2.5h spent, 2.5h effective, contribution=2.25\n  ml_courses: 1.0h spent, 1.0h effective, contribution=1.00\n  social_media: 2.0h spent, 2.0h effective, contribution=0.20\n  exercise: 0.8h spent, 0.8h effective, contribution=0.53\n  news: 0.5h spent, 0.5h effective, contribution=0.15\n</code></pre>"},{"location":"quickstart/#using-individual-metrics","title":"Using Individual Metrics","text":"<p>You can also call metrics directly without generating a full report:</p> <pre><code>from meaningful_metrics import (\n    calculate_quality_time_score,\n    calculate_goal_alignment,\n    calculate_distraction_ratio,\n    calculate_actionability_score,\n)\n\n# Just the QTS\nqts = calculate_quality_time_score(entries, priorities)\n\n# Goal alignment\nalignment = calculate_goal_alignment(entries, goals)\n\n# Actionability (requires action log)\nfrom meaningful_metrics.schemas import ActionLog\n\nactions = ActionLog(consumed=50, bookmarked=8, shared=3, applied=5)\nactionability = calculate_actionability_score(\n    consumed=actions.consumed,\n    bookmarked=actions.bookmarked,\n    shared=actions.shared,\n    applied=actions.applied,\n)\nprint(f\"Actionability Score: {actionability:.3f}\")\n# Actionability Score: 0.224\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Concepts \u2014 Understand the math behind each metric</li> <li>API Reference \u2014 Full function signatures</li> <li>Case Studies \u2014 See how to evaluate an AI product</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides complete API documentation for the <code>meaningful_metrics</code> package, auto-generated from docstrings.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>meaningful_metrics/\n\u251c\u2500\u2500 __init__.py       # Public API exports\n\u251c\u2500\u2500 metrics.py        # Core metric calculation functions\n\u251c\u2500\u2500 schemas.py        # Pydantic data models\n\u2514\u2500\u2500 scoring.py        # Composite scoring and report generation\n</code></pre>"},{"location":"api/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Metrics \u2014 Core calculation functions (<code>calculate_quality_time_score</code>, <code>calculate_goal_alignment</code>, etc.)</li> <li>Schemas \u2014 Data models (<code>Goal</code>, <code>TimeEntry</code>, <code>DomainPriority</code>, <code>MetricsReport</code>, etc.)</li> <li>Scoring \u2014 Report generation (<code>generate_metrics_report</code>, <code>generate_recommendations</code>, etc.)</li> </ul>"},{"location":"api/#top-level-exports","title":"Top-Level Exports","text":"<p>All public functions and types are available directly from the <code>meaningful_metrics</code> namespace:</p> <pre><code>from meaningful_metrics import (\n    # Core metrics\n    calculate_quality_time_score,\n    calculate_goal_alignment,\n    calculate_distraction_ratio,\n    calculate_actionability_score,\n    calculate_locality_score,\n    # Composite scoring\n    generate_metrics_report,\n    generate_recommendations,\n    calculate_domain_contributions,\n    # Data models\n    Goal,\n    DomainPriority,\n    TimeEntry,\n    ActionLog,\n    ActionWeights,\n    DomainMetrics,\n    Recommendation,\n    MetricsReport,\n)\n</code></pre>"},{"location":"api/metrics/","title":"Metrics API","text":"<p>Core metric calculation functions. All functions are pure \u2014 no side effects, no state.</p>"},{"location":"api/metrics/#calculate_quality_time_score","title":"calculate_quality_time_score","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_quality_time_score","title":"meaningful_metrics.metrics.calculate_quality_time_score","text":"<pre><code>calculate_quality_time_score(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    *,\n    default_priority: float = 0.5\n) -&gt; float\n</code></pre> <p>Calculate the Quality Time Score from time entries and priorities.</p> <p>The QTS weights time spent by priority and applies diminishing returns caps to prevent over-optimization on any single domain.</p> Formula <p>QTS = sum(min(Ti, Capi) * Pi)</p> <p>Where: - Ti = Time spent in domain i - Capi = Max valuable hours for domain i (or infinity if not set) - Pi = Priority score for domain i (0.0 - 1.0)</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>priorities</code> <code>list[DomainPriority]</code> <p>List of domain priorities with optional caps.</p> required <code>default_priority</code> <code>float</code> <p>Priority for domains without explicit priority (default: 0.5).</p> <code>0.5</code> <p>Returns:</p> Type Description <code>float</code> <p>The calculated Quality Time Score as a float.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If time entries contain negative hours.</p> Example <p>from meaningful_metrics.schemas import TimeEntry, DomainPriority entries = [TimeEntry(domain=\"learning\", hours=2.0)] priorities = [DomainPriority(domain=\"learning\", priority=1.0)] calculate_quality_time_score(entries, priorities) 2.0</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_quality_time_score(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    *,\n    default_priority: float = 0.5,\n) -&gt; float:\n    \"\"\"Calculate the Quality Time Score from time entries and priorities.\n\n    The QTS weights time spent by priority and applies diminishing returns\n    caps to prevent over-optimization on any single domain.\n\n    Formula:\n        QTS = sum(min(Ti, Capi) * Pi)\n\n        Where:\n        - Ti = Time spent in domain i\n        - Capi = Max valuable hours for domain i (or infinity if not set)\n        - Pi = Priority score for domain i (0.0 - 1.0)\n\n    Args:\n        time_entries: List of time spent per domain.\n        priorities: List of domain priorities with optional caps.\n        default_priority: Priority for domains without explicit priority (default: 0.5).\n\n    Returns:\n        The calculated Quality Time Score as a float.\n\n    Raises:\n        ValueError: If time entries contain negative hours.\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import TimeEntry, DomainPriority\n        &gt;&gt;&gt; entries = [TimeEntry(domain=\"learning\", hours=2.0)]\n        &gt;&gt;&gt; priorities = [DomainPriority(domain=\"learning\", priority=1.0)]\n        &gt;&gt;&gt; calculate_quality_time_score(entries, priorities)\n        2.0\n\n        &gt;&gt;&gt; # With diminishing returns cap\n        &gt;&gt;&gt; priorities = [DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=1.0)]\n        &gt;&gt;&gt; calculate_quality_time_score(entries, priorities)\n        1.0\n    \"\"\"\n    # Build priority lookup\n    priority_map: dict[str, DomainPriority] = {p.domain: p for p in priorities}\n\n    qts = 0.0\n\n    for entry in time_entries:\n        if entry.hours &lt; 0:\n            msg = f\"Negative hours not allowed: {entry.domain} = {entry.hours}\"\n            raise ValueError(msg)\n\n        # Get priority settings for this domain\n        priority_config = priority_map.get(entry.domain)\n\n        if priority_config:\n            priority = priority_config.priority\n            cap = priority_config.max_daily_hours\n        else:\n            priority = default_priority\n            cap = None\n\n        # Apply diminishing returns cap\n        effective_time = entry.hours if cap is None else min(entry.hours, cap)\n\n        # Add weighted contribution\n        qts += effective_time * priority\n\n    return qts\n</code></pre>"},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_quality_time_score--with-diminishing-returns-cap","title":"With diminishing returns cap","text":"<p>priorities = [DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=1.0)] calculate_quality_time_score(entries, priorities) 1.0</p>"},{"location":"api/metrics/#calculate_goal_alignment","title":"calculate_goal_alignment","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_goal_alignment","title":"meaningful_metrics.metrics.calculate_goal_alignment","text":"<pre><code>calculate_goal_alignment(\n    time_entries: list[TimeEntry], goals: list[Goal]\n) -&gt; float\n</code></pre> <p>Calculate goal alignment percentage.</p> <p>Goal Alignment measures what percentage of tracked time is spent on activities that directly support the user's stated goals.</p> Formula <p>GA = (sum(Time_goal_domains) / Total_time) * 100</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>goals</code> <code>list[Goal]</code> <p>List of user's goals with linked domains.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Percentage of time spent on goal-related activities (0.0 to 100.0).</p> Example <p>from meaningful_metrics.schemas import TimeEntry, Goal entries = [ ...     TimeEntry(domain=\"learning\", hours=2.0), ...     TimeEntry(domain=\"social_media\", hours=3.0), ... ] goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])] calculate_goal_alignment(entries, goals) 40.0</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_goal_alignment(\n    time_entries: list[TimeEntry],\n    goals: list[Goal],\n) -&gt; float:\n    \"\"\"Calculate goal alignment percentage.\n\n    Goal Alignment measures what percentage of tracked time is spent on\n    activities that directly support the user's stated goals.\n\n    Formula:\n        GA = (sum(Time_goal_domains) / Total_time) * 100\n\n    Args:\n        time_entries: List of time spent per domain.\n        goals: List of user's goals with linked domains.\n\n    Returns:\n        Percentage of time spent on goal-related activities (0.0 to 100.0).\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import TimeEntry, Goal\n        &gt;&gt;&gt; entries = [\n        ...     TimeEntry(domain=\"learning\", hours=2.0),\n        ...     TimeEntry(domain=\"social_media\", hours=3.0),\n        ... ]\n        &gt;&gt;&gt; goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])]\n        &gt;&gt;&gt; calculate_goal_alignment(entries, goals)\n        40.0\n    \"\"\"\n    if not time_entries:\n        return 0.0\n\n    # Collect all goal-related domains\n    goal_domains: set[str] = set()\n    for goal in goals:\n        goal_domains.update(goal.domains)\n\n    # Calculate totals\n    total_time = sum(entry.hours for entry in time_entries)\n    goal_time = sum(\n        entry.hours for entry in time_entries if entry.domain in goal_domains\n    )\n\n    if total_time == 0:\n        return 0.0\n\n    return (goal_time / total_time) * 100\n</code></pre>"},{"location":"api/metrics/#calculate_distraction_ratio","title":"calculate_distraction_ratio","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_distraction_ratio","title":"meaningful_metrics.metrics.calculate_distraction_ratio","text":"<pre><code>calculate_distraction_ratio(\n    time_entries: list[TimeEntry], goals: list[Goal]\n) -&gt; float\n</code></pre> <p>Calculate distraction ratio percentage.</p> <p>The Distraction Ratio is the inverse of Goal Alignment - it measures what percentage of time is spent on non-goal activities.</p> Formula <p>DR = 100 - Goal_Alignment</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>goals</code> <code>list[Goal]</code> <p>List of user's goals with linked domains.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Percentage of time spent on non-goal activities (0.0 to 100.0).</p> Example <p>from meaningful_metrics.schemas import TimeEntry, Goal entries = [ ...     TimeEntry(domain=\"learning\", hours=2.0), ...     TimeEntry(domain=\"social_media\", hours=3.0), ... ] goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])] calculate_distraction_ratio(entries, goals) 60.0</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_distraction_ratio(\n    time_entries: list[TimeEntry],\n    goals: list[Goal],\n) -&gt; float:\n    \"\"\"Calculate distraction ratio percentage.\n\n    The Distraction Ratio is the inverse of Goal Alignment - it measures\n    what percentage of time is spent on non-goal activities.\n\n    Formula:\n        DR = 100 - Goal_Alignment\n\n    Args:\n        time_entries: List of time spent per domain.\n        goals: List of user's goals with linked domains.\n\n    Returns:\n        Percentage of time spent on non-goal activities (0.0 to 100.0).\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import TimeEntry, Goal\n        &gt;&gt;&gt; entries = [\n        ...     TimeEntry(domain=\"learning\", hours=2.0),\n        ...     TimeEntry(domain=\"social_media\", hours=3.0),\n        ... ]\n        &gt;&gt;&gt; goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])]\n        &gt;&gt;&gt; calculate_distraction_ratio(entries, goals)\n        60.0\n    \"\"\"\n    return 100.0 - calculate_goal_alignment(time_entries, goals)\n</code></pre>"},{"location":"api/metrics/#calculate_actionability_score","title":"calculate_actionability_score","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_actionability_score","title":"meaningful_metrics.metrics.calculate_actionability_score","text":"<pre><code>calculate_actionability_score(\n    consumed: int,\n    bookmarked: int = 0,\n    shared: int = 0,\n    applied: int = 0,\n    weights: ActionWeights | None = None,\n) -&gt; float\n</code></pre> <p>Calculate actionability score from content consumption and actions.</p> <p>The Actionability Score measures how effectively consumed information translates into meaningful action. Higher scores indicate more action-oriented consumption.</p> Formula <p>AS = (bookmarked * w_b + shared * w_s + applied * w_a) / consumed</p> <p>Default weights: w_b=0.3, w_s=0.5, w_a=1.0</p> <p>Parameters:</p> Name Type Description Default <code>consumed</code> <code>int</code> <p>Total items consumed (articles, videos, etc.).</p> required <code>bookmarked</code> <code>int</code> <p>Items saved for later (default: 0).</p> <code>0</code> <code>shared</code> <code>int</code> <p>Items shared with others (default: 0).</p> <code>0</code> <code>applied</code> <code>int</code> <p>Items that led to concrete action (default: 0).</p> <code>0</code> <code>weights</code> <code>ActionWeights | None</code> <p>Custom ActionWeights (default: standard weights).</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Actionability score as a float (typically 0.0 to 1.0, can exceed 1.0).</p> Example <p>calculate_actionability_score(consumed=100, bookmarked=20, shared=5, applied=10) 0.185</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_actionability_score(\n    consumed: int,\n    bookmarked: int = 0,\n    shared: int = 0,\n    applied: int = 0,\n    weights: ActionWeights | None = None,\n) -&gt; float:\n    \"\"\"Calculate actionability score from content consumption and actions.\n\n    The Actionability Score measures how effectively consumed information\n    translates into meaningful action. Higher scores indicate more\n    action-oriented consumption.\n\n    Formula:\n        AS = (bookmarked * w_b + shared * w_s + applied * w_a) / consumed\n\n        Default weights: w_b=0.3, w_s=0.5, w_a=1.0\n\n    Args:\n        consumed: Total items consumed (articles, videos, etc.).\n        bookmarked: Items saved for later (default: 0).\n        shared: Items shared with others (default: 0).\n        applied: Items that led to concrete action (default: 0).\n        weights: Custom ActionWeights (default: standard weights).\n\n    Returns:\n        Actionability score as a float (typically 0.0 to 1.0, can exceed 1.0).\n\n    Example:\n        &gt;&gt;&gt; calculate_actionability_score(consumed=100, bookmarked=20, shared=5, applied=10)\n        0.185\n    \"\"\"\n    if consumed == 0:\n        return 0.0\n\n    # Use default weights if not provided\n    if weights is None:\n        from meaningful_metrics.schemas import ActionWeights\n\n        weights = ActionWeights()\n\n    weighted_actions = (\n        bookmarked * weights.bookmarked\n        + shared * weights.shared\n        + applied * weights.applied\n    )\n\n    return weighted_actions / consumed\n</code></pre>"},{"location":"api/metrics/#calculate_actionability_score_from_log","title":"calculate_actionability_score_from_log","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_actionability_score_from_log","title":"meaningful_metrics.metrics.calculate_actionability_score_from_log","text":"<pre><code>calculate_actionability_score_from_log(\n    action_log: ActionLog,\n    weights: ActionWeights | None = None,\n) -&gt; float\n</code></pre> <p>Calculate actionability score from an ActionLog object.</p> <p>Convenience function that unpacks an ActionLog and calls calculate_actionability_score.</p> <p>Parameters:</p> Name Type Description Default <code>action_log</code> <code>ActionLog</code> <p>ActionLog containing consumption and action data.</p> required <code>weights</code> <code>ActionWeights | None</code> <p>Custom ActionWeights (default: standard weights).</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Actionability score as a float.</p> Example <p>from meaningful_metrics.schemas import ActionLog log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10) calculate_actionability_score_from_log(log) 0.185</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_actionability_score_from_log(\n    action_log: ActionLog,\n    weights: ActionWeights | None = None,\n) -&gt; float:\n    \"\"\"Calculate actionability score from an ActionLog object.\n\n    Convenience function that unpacks an ActionLog and calls\n    calculate_actionability_score.\n\n    Args:\n        action_log: ActionLog containing consumption and action data.\n        weights: Custom ActionWeights (default: standard weights).\n\n    Returns:\n        Actionability score as a float.\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import ActionLog\n        &gt;&gt;&gt; log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10)\n        &gt;&gt;&gt; calculate_actionability_score_from_log(log)\n        0.185\n    \"\"\"\n    return calculate_actionability_score(\n        consumed=action_log.consumed,\n        bookmarked=action_log.bookmarked,\n        shared=action_log.shared,\n        applied=action_log.applied,\n        weights=weights,\n    )\n</code></pre>"},{"location":"api/metrics/#calculate_locality_score","title":"calculate_locality_score","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.calculate_locality_score","title":"meaningful_metrics.metrics.calculate_locality_score","text":"<pre><code>calculate_locality_score(\n    local_relevance: float, engagement: float\n) -&gt; float\n</code></pre> <p>Calculate locality score for community-relevant content.</p> <p>The Locality Score weights content by its relevance to the user's local community, multiplied by engagement level.</p> Formula <p>LS = local_relevance * engagement</p> <p>Parameters:</p> Name Type Description Default <code>local_relevance</code> <code>float</code> <p>Relevance to local community (0.0 to 1.0).</p> required <code>engagement</code> <code>float</code> <p>User's engagement with the content (0.0 to 1.0).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Locality score as a float (0.0 to 1.0).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are outside valid range.</p> Example <p>calculate_locality_score(local_relevance=0.8, engagement=0.6) 0.48</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def calculate_locality_score(\n    local_relevance: float,\n    engagement: float,\n) -&gt; float:\n    \"\"\"Calculate locality score for community-relevant content.\n\n    The Locality Score weights content by its relevance to the user's\n    local community, multiplied by engagement level.\n\n    Formula:\n        LS = local_relevance * engagement\n\n    Args:\n        local_relevance: Relevance to local community (0.0 to 1.0).\n        engagement: User's engagement with the content (0.0 to 1.0).\n\n    Returns:\n        Locality score as a float (0.0 to 1.0).\n\n    Raises:\n        ValueError: If inputs are outside valid range.\n\n    Example:\n        &gt;&gt;&gt; calculate_locality_score(local_relevance=0.8, engagement=0.6)\n        0.48\n    \"\"\"\n    if not 0.0 &lt;= local_relevance &lt;= 1.0:\n        msg = f\"local_relevance must be between 0.0 and 1.0, got {local_relevance}\"\n        raise ValueError(msg)\n\n    if not 0.0 &lt;= engagement &lt;= 1.0:\n        msg = f\"engagement must be between 0.0 and 1.0, got {engagement}\"\n        raise ValueError(msg)\n\n    return local_relevance * engagement\n</code></pre>"},{"location":"api/metrics/#soft_min","title":"soft_min","text":""},{"location":"api/metrics/#meaningful_metrics.metrics.soft_min","title":"meaningful_metrics.metrics.soft_min","text":"<pre><code>soft_min(a: float, b: float, alpha: float = 10.0) -&gt; float\n</code></pre> <p>Smooth approximation of min(a, b) for differentiable optimization.</p> <p>Uses the log-sum-exp trick to create a differentiable approximation of the minimum function. Useful for ML optimization where gradients are needed.</p> Formula <p>soft_min(a, b) = -1/alpha * log(exp(-alpha * a) + exp(-alpha * b))</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>First value.</p> required <code>b</code> <code>float</code> <p>Second value.</p> required <code>alpha</code> <code>float</code> <p>Smoothing parameter (higher = closer to true min).</p> <code>10.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Smooth approximation of min(a, b).</p> Example <p>soft_min(2.0, 3.0)  # Close to 2.0 1.9999...</p> Source code in <code>src/meaningful_metrics/metrics.py</code> <pre><code>def soft_min(a: float, b: float, alpha: float = 10.0) -&gt; float:\n    \"\"\"Smooth approximation of min(a, b) for differentiable optimization.\n\n    Uses the log-sum-exp trick to create a differentiable approximation\n    of the minimum function. Useful for ML optimization where gradients\n    are needed.\n\n    Formula:\n        soft_min(a, b) = -1/alpha * log(exp(-alpha * a) + exp(-alpha * b))\n\n    Args:\n        a: First value.\n        b: Second value.\n        alpha: Smoothing parameter (higher = closer to true min).\n\n    Returns:\n        Smooth approximation of min(a, b).\n\n    Example:\n        &gt;&gt;&gt; soft_min(2.0, 3.0)  # Close to 2.0\n        1.9999...\n    \"\"\"\n    # Numerical stability: subtract max before exp\n    max_val = max(-alpha * a, -alpha * b)\n    return (\n        -1\n        / alpha\n        * (\n            max_val\n            + math.log(math.exp(-alpha * a - max_val) + math.exp(-alpha * b - max_val))\n        )\n    )\n</code></pre>"},{"location":"api/schemas/","title":"Schemas API","text":"<p>Pydantic data models for input and output types. All models validate inputs at construction time.</p>"},{"location":"api/schemas/#input-types","title":"Input Types","text":""},{"location":"api/schemas/#goal","title":"Goal","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.Goal","title":"meaningful_metrics.schemas.Goal","text":"<p>               Bases: <code>BaseModel</code></p> <p>A user's goal with linked content domains.</p> <p>Goals define what the user wants to achieve and which content domains support that goal. This is used to calculate Goal Alignment.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the goal.</p> <code>name</code> <code>str</code> <p>Human-readable name for the goal.</p> <code>domains</code> <code>list[str]</code> <p>List of content domains that support this goal.</p> <code>target_hours_per_week</code> <code>float | None</code> <p>Optional weekly time target for this goal.</p> Example <p>goal = Goal( ...     id=\"learn-spanish\", ...     name=\"Learn Spanish\", ...     domains=[\"language_learning\", \"spanish_media\"], ...     target_hours_per_week=7.0, ... )</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class Goal(BaseModel):\n    \"\"\"A user's goal with linked content domains.\n\n    Goals define what the user wants to achieve and which content domains\n    support that goal. This is used to calculate Goal Alignment.\n\n    Attributes:\n        id: Unique identifier for the goal.\n        name: Human-readable name for the goal.\n        domains: List of content domains that support this goal.\n        target_hours_per_week: Optional weekly time target for this goal.\n\n    Example:\n        &gt;&gt;&gt; goal = Goal(\n        ...     id=\"learn-spanish\",\n        ...     name=\"Learn Spanish\",\n        ...     domains=[\"language_learning\", \"spanish_media\"],\n        ...     target_hours_per_week=7.0,\n        ... )\n    \"\"\"\n\n    id: str = Field(..., min_length=1, description=\"Unique identifier for the goal\")\n    name: str = Field(..., min_length=1, description=\"Human-readable goal name\")\n    domains: list[str] = Field(\n        default_factory=list,\n        description=\"Content domains that support this goal\",\n    )\n    target_hours_per_week: float | None = Field(\n        default=None,\n        ge=0,\n        description=\"Optional weekly time target in hours\",\n    )\n</code></pre>"},{"location":"api/schemas/#domainpriority","title":"DomainPriority","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.DomainPriority","title":"meaningful_metrics.schemas.DomainPriority","text":"<p>               Bases: <code>BaseModel</code></p> <p>Priority and cap settings for a content domain.</p> <p>Domain priorities weight time spent in different activities according to user preferences. The optional cap implements diminishing returns.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>The content domain identifier (e.g., \"learning\", \"social_media\").</p> <code>priority</code> <code>float</code> <p>Priority weight from 0.0 (lowest) to 1.0 (highest).</p> <code>max_daily_hours</code> <code>float | None</code> <p>Optional cap for diminishing returns.</p> Example <p>priority = DomainPriority( ...     domain=\"learning\", ...     priority=1.0, ...     max_daily_hours=4.0, ... )</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class DomainPriority(BaseModel):\n    \"\"\"Priority and cap settings for a content domain.\n\n    Domain priorities weight time spent in different activities according\n    to user preferences. The optional cap implements diminishing returns.\n\n    Attributes:\n        domain: The content domain identifier (e.g., \"learning\", \"social_media\").\n        priority: Priority weight from 0.0 (lowest) to 1.0 (highest).\n        max_daily_hours: Optional cap for diminishing returns.\n\n    Example:\n        &gt;&gt;&gt; priority = DomainPriority(\n        ...     domain=\"learning\",\n        ...     priority=1.0,\n        ...     max_daily_hours=4.0,\n        ... )\n    \"\"\"\n\n    domain: str = Field(..., min_length=1, description=\"Content domain identifier\")\n    priority: float = Field(\n        ...,\n        ge=0.0,\n        le=1.0,\n        description=\"Priority weight (0.0 to 1.0)\",\n    )\n    max_daily_hours: float | None = Field(\n        default=None,\n        gt=0,\n        description=\"Maximum valuable hours per day (diminishing returns cap)\",\n    )\n\n    @field_validator(\"priority\")\n    @classmethod\n    def validate_priority(cls, v: float) -&gt; float:\n        \"\"\"Validate that the priority value is within the acceptable range.\n\n        Args:\n            v: The priority value to validate. Must be between 0.0 and 1.0.\n\n        Returns:\n            The validated priority value unchanged.\n\n        Raises:\n            ValueError: If priority is outside the range [0.0, 1.0].\n\n        Example:\n            &gt;&gt;&gt; DomainPriority(domain=\"learning\", priority=0.8)\n            DomainPriority(domain='learning', priority=0.8, max_daily_hours=None)\n            &gt;&gt;&gt; DomainPriority(domain=\"learning\", priority=1.5)  # raises ValueError\n        \"\"\"\n        if not 0.0 &lt;= v &lt;= 1.0:\n            msg = \"Priority must be between 0.0 and 1.0\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"api/schemas/#meaningful_metrics.schemas.DomainPriority.validate_priority","title":"validate_priority  <code>classmethod</code>","text":"<pre><code>validate_priority(v: float) -&gt; float\n</code></pre> <p>Validate that the priority value is within the acceptable range.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>float</code> <p>The priority value to validate. Must be between 0.0 and 1.0.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The validated priority value unchanged.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If priority is outside the range [0.0, 1.0].</p> Example <p>DomainPriority(domain=\"learning\", priority=0.8) DomainPriority(domain='learning', priority=0.8, max_daily_hours=None) DomainPriority(domain=\"learning\", priority=1.5)  # raises ValueError</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>@field_validator(\"priority\")\n@classmethod\ndef validate_priority(cls, v: float) -&gt; float:\n    \"\"\"Validate that the priority value is within the acceptable range.\n\n    Args:\n        v: The priority value to validate. Must be between 0.0 and 1.0.\n\n    Returns:\n        The validated priority value unchanged.\n\n    Raises:\n        ValueError: If priority is outside the range [0.0, 1.0].\n\n    Example:\n        &gt;&gt;&gt; DomainPriority(domain=\"learning\", priority=0.8)\n        DomainPriority(domain='learning', priority=0.8, max_daily_hours=None)\n        &gt;&gt;&gt; DomainPriority(domain=\"learning\", priority=1.5)  # raises ValueError\n    \"\"\"\n    if not 0.0 &lt;= v &lt;= 1.0:\n        msg = \"Priority must be between 0.0 and 1.0\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"api/schemas/#timeentry","title":"TimeEntry","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.TimeEntry","title":"meaningful_metrics.schemas.TimeEntry","text":"<p>               Bases: <code>BaseModel</code></p> <p>Time spent in a content domain.</p> <p>Represents tracked time for metric calculations.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>The content domain identifier.</p> <code>hours</code> <code>float</code> <p>Time spent in hours (must be non-negative).</p> Example <p>entry = TimeEntry(domain=\"learning\", hours=2.5)</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class TimeEntry(BaseModel):\n    \"\"\"Time spent in a content domain.\n\n    Represents tracked time for metric calculations.\n\n    Attributes:\n        domain: The content domain identifier.\n        hours: Time spent in hours (must be non-negative).\n\n    Example:\n        &gt;&gt;&gt; entry = TimeEntry(domain=\"learning\", hours=2.5)\n    \"\"\"\n\n    domain: str = Field(..., min_length=1, description=\"Content domain identifier\")\n    hours: float = Field(..., ge=0, description=\"Time spent in hours\")\n\n    @field_validator(\"hours\")\n    @classmethod\n    def validate_hours(cls, v: float) -&gt; float:\n        \"\"\"Validate that time spent in hours is non-negative.\n\n        Args:\n            v: The hours value to validate. Must be &gt;= 0.\n\n        Returns:\n            The validated hours value unchanged.\n\n        Raises:\n            ValueError: If hours is negative.\n\n        Example:\n            &gt;&gt;&gt; TimeEntry(domain=\"learning\", hours=2.5)\n            TimeEntry(domain='learning', hours=2.5)\n            &gt;&gt;&gt; TimeEntry(domain=\"learning\", hours=-1.0)  # raises ValueError\n        \"\"\"\n        if v &lt; 0:\n            msg = \"Hours cannot be negative\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"api/schemas/#meaningful_metrics.schemas.TimeEntry.validate_hours","title":"validate_hours  <code>classmethod</code>","text":"<pre><code>validate_hours(v: float) -&gt; float\n</code></pre> <p>Validate that time spent in hours is non-negative.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>float</code> <p>The hours value to validate. Must be &gt;= 0.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The validated hours value unchanged.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hours is negative.</p> Example <p>TimeEntry(domain=\"learning\", hours=2.5) TimeEntry(domain='learning', hours=2.5) TimeEntry(domain=\"learning\", hours=-1.0)  # raises ValueError</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>@field_validator(\"hours\")\n@classmethod\ndef validate_hours(cls, v: float) -&gt; float:\n    \"\"\"Validate that time spent in hours is non-negative.\n\n    Args:\n        v: The hours value to validate. Must be &gt;= 0.\n\n    Returns:\n        The validated hours value unchanged.\n\n    Raises:\n        ValueError: If hours is negative.\n\n    Example:\n        &gt;&gt;&gt; TimeEntry(domain=\"learning\", hours=2.5)\n        TimeEntry(domain='learning', hours=2.5)\n        &gt;&gt;&gt; TimeEntry(domain=\"learning\", hours=-1.0)  # raises ValueError\n    \"\"\"\n    if v &lt; 0:\n        msg = \"Hours cannot be negative\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"api/schemas/#actionlog","title":"ActionLog","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.ActionLog","title":"meaningful_metrics.schemas.ActionLog","text":"<p>               Bases: <code>BaseModel</code></p> <p>Log of actions taken on consumed content.</p> <p>Used to calculate the Actionability Score.</p> <p>Attributes:</p> Name Type Description <code>consumed</code> <code>int</code> <p>Total items consumed (articles, videos, etc.).</p> <code>bookmarked</code> <code>int</code> <p>Items saved for later.</p> <code>shared</code> <code>int</code> <p>Items shared with others.</p> <code>applied</code> <code>int</code> <p>Items that led to concrete action.</p> Example <p>log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10)</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class ActionLog(BaseModel):\n    \"\"\"Log of actions taken on consumed content.\n\n    Used to calculate the Actionability Score.\n\n    Attributes:\n        consumed: Total items consumed (articles, videos, etc.).\n        bookmarked: Items saved for later.\n        shared: Items shared with others.\n        applied: Items that led to concrete action.\n\n    Example:\n        &gt;&gt;&gt; log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10)\n    \"\"\"\n\n    consumed: int = Field(..., ge=0, description=\"Total items consumed\")\n    bookmarked: int = Field(default=0, ge=0, description=\"Items saved for later\")\n    shared: int = Field(default=0, ge=0, description=\"Items shared with others\")\n    applied: int = Field(default=0, ge=0, description=\"Items that led to action\")\n\n    @field_validator(\"bookmarked\", \"shared\", \"applied\")\n    @classmethod\n    def validate_not_exceeds_consumed(cls, v: int) -&gt; int:\n        \"\"\"Validate action count fields for the action log.\n\n        Args:\n            v: The action count value (bookmarked, shared, or applied). Must be &gt;= 0.\n\n        Returns:\n            The validated count value unchanged.\n\n        Note:\n            This is a soft validation \u2014 items can be acted on in multiple ways,\n            so action counts are not strictly bounded by consumed count.\n\n        Example:\n            &gt;&gt;&gt; log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10)\n            &gt;&gt;&gt; log.bookmarked\n            20\n        \"\"\"\n        # Note: This is a soft validation - items can be acted on multiple ways\n        return v\n</code></pre>"},{"location":"api/schemas/#meaningful_metrics.schemas.ActionLog.validate_not_exceeds_consumed","title":"validate_not_exceeds_consumed  <code>classmethod</code>","text":"<pre><code>validate_not_exceeds_consumed(v: int) -&gt; int\n</code></pre> <p>Validate action count fields for the action log.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>The action count value (bookmarked, shared, or applied). Must be &gt;= 0.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The validated count value unchanged.</p> Note <p>This is a soft validation \u2014 items can be acted on in multiple ways, so action counts are not strictly bounded by consumed count.</p> Example <p>log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10) log.bookmarked 20</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>@field_validator(\"bookmarked\", \"shared\", \"applied\")\n@classmethod\ndef validate_not_exceeds_consumed(cls, v: int) -&gt; int:\n    \"\"\"Validate action count fields for the action log.\n\n    Args:\n        v: The action count value (bookmarked, shared, or applied). Must be &gt;= 0.\n\n    Returns:\n        The validated count value unchanged.\n\n    Note:\n        This is a soft validation \u2014 items can be acted on in multiple ways,\n        so action counts are not strictly bounded by consumed count.\n\n    Example:\n        &gt;&gt;&gt; log = ActionLog(consumed=100, bookmarked=20, shared=5, applied=10)\n        &gt;&gt;&gt; log.bookmarked\n        20\n    \"\"\"\n    # Note: This is a soft validation - items can be acted on multiple ways\n    return v\n</code></pre>"},{"location":"api/schemas/#actionweights","title":"ActionWeights","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.ActionWeights","title":"meaningful_metrics.schemas.ActionWeights","text":"<p>               Bases: <code>BaseModel</code></p> <p>Custom weights for actionability score calculation.</p> <p>Allows users to customize the relative importance of different actions.</p> <p>Attributes:</p> Name Type Description <code>bookmarked</code> <code>float</code> <p>Weight for bookmarked items (default: 0.3).</p> <code>shared</code> <code>float</code> <p>Weight for shared items (default: 0.5).</p> <code>applied</code> <code>float</code> <p>Weight for applied items (default: 1.0).</p> Example <p>weights = ActionWeights(bookmarked=0.2, shared=0.4, applied=1.5)</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class ActionWeights(BaseModel):\n    \"\"\"Custom weights for actionability score calculation.\n\n    Allows users to customize the relative importance of different actions.\n\n    Attributes:\n        bookmarked: Weight for bookmarked items (default: 0.3).\n        shared: Weight for shared items (default: 0.5).\n        applied: Weight for applied items (default: 1.0).\n\n    Example:\n        &gt;&gt;&gt; weights = ActionWeights(bookmarked=0.2, shared=0.4, applied=1.5)\n    \"\"\"\n\n    bookmarked: float = Field(default=0.3, ge=0, description=\"Weight for bookmarks\")\n    shared: float = Field(default=0.5, ge=0, description=\"Weight for shares\")\n    applied: float = Field(default=1.0, ge=0, description=\"Weight for applications\")\n</code></pre>"},{"location":"api/schemas/#output-types","title":"Output Types","text":""},{"location":"api/schemas/#domainmetrics","title":"DomainMetrics","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.DomainMetrics","title":"meaningful_metrics.schemas.DomainMetrics","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metrics for a single content domain.</p> <p>Provides detailed breakdown of how a domain contributes to overall metrics.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>The content domain identifier.</p> <code>time_spent</code> <code>float</code> <p>Raw time spent in hours.</p> <code>effective_time</code> <code>float</code> <p>Time after applying diminishing returns cap.</p> <code>priority</code> <code>float</code> <p>The domain's priority weight.</p> <code>contribution</code> <code>float</code> <p>Contribution to Quality Time Score.</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class DomainMetrics(BaseModel):\n    \"\"\"Metrics for a single content domain.\n\n    Provides detailed breakdown of how a domain contributes to overall metrics.\n\n    Attributes:\n        domain: The content domain identifier.\n        time_spent: Raw time spent in hours.\n        effective_time: Time after applying diminishing returns cap.\n        priority: The domain's priority weight.\n        contribution: Contribution to Quality Time Score.\n    \"\"\"\n\n    domain: str = Field(..., description=\"Content domain identifier\")\n    time_spent: float = Field(..., ge=0, description=\"Raw time spent in hours\")\n    effective_time: float = Field(\n        ...,\n        ge=0,\n        description=\"Time after diminishing returns cap\",\n    )\n    priority: float = Field(..., ge=0, le=1, description=\"Domain priority weight\")\n    contribution: float = Field(..., ge=0, description=\"Contribution to QTS\")\n</code></pre>"},{"location":"api/schemas/#recommendation","title":"Recommendation","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.Recommendation","title":"meaningful_metrics.schemas.Recommendation","text":"<p>               Bases: <code>BaseModel</code></p> <p>A suggested action for the user.</p> <p>Recommendations are generated based on metrics analysis to help users improve their time allocation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['increase', 'decrease', 'maintain']</code> <p>Whether to increase, decrease, or maintain time in domain.</p> <code>domain</code> <code>str</code> <p>The content domain this recommendation applies to.</p> <code>message</code> <code>str</code> <p>Human-readable recommendation message.</p> <code>priority</code> <code>Literal['high', 'medium', 'low']</code> <p>Importance level of this recommendation.</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class Recommendation(BaseModel):\n    \"\"\"A suggested action for the user.\n\n    Recommendations are generated based on metrics analysis to help\n    users improve their time allocation.\n\n    Attributes:\n        type: Whether to increase, decrease, or maintain time in domain.\n        domain: The content domain this recommendation applies to.\n        message: Human-readable recommendation message.\n        priority: Importance level of this recommendation.\n    \"\"\"\n\n    type: Literal[\"increase\", \"decrease\", \"maintain\"] = Field(\n        ...,\n        description=\"Action type\",\n    )\n    domain: str = Field(..., description=\"Target content domain\")\n    message: str = Field(..., description=\"Human-readable recommendation\")\n    priority: Literal[\"high\", \"medium\", \"low\"] = Field(\n        ...,\n        description=\"Recommendation importance\",\n    )\n</code></pre>"},{"location":"api/schemas/#metricsreport","title":"MetricsReport","text":""},{"location":"api/schemas/#meaningful_metrics.schemas.MetricsReport","title":"meaningful_metrics.schemas.MetricsReport","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete metrics report for a time period.</p> <p>Aggregates all metrics and provides actionable recommendations.</p> <p>Attributes:</p> Name Type Description <code>period</code> <code>Literal['daily', 'weekly']</code> <p>The reporting period (daily or weekly).</p> <code>quality_time_score</code> <code>float</code> <p>The calculated QTS.</p> <code>raw_time_hours</code> <code>float</code> <p>Total raw time tracked.</p> <code>goal_alignment_percent</code> <code>float</code> <p>Percentage of time on goal-related activities.</p> <code>distraction_percent</code> <code>float</code> <p>Percentage of time on non-goal activities.</p> <code>actionability_score</code> <code>float</code> <p>Information to action conversion rate.</p> <code>by_domain</code> <code>list[DomainMetrics]</code> <p>Breakdown of metrics by domain.</p> <code>recommendations</code> <code>list[Recommendation]</code> <p>Suggested actions for improvement.</p> Source code in <code>src/meaningful_metrics/schemas.py</code> <pre><code>class MetricsReport(BaseModel):\n    \"\"\"Complete metrics report for a time period.\n\n    Aggregates all metrics and provides actionable recommendations.\n\n    Attributes:\n        period: The reporting period (daily or weekly).\n        quality_time_score: The calculated QTS.\n        raw_time_hours: Total raw time tracked.\n        goal_alignment_percent: Percentage of time on goal-related activities.\n        distraction_percent: Percentage of time on non-goal activities.\n        actionability_score: Information to action conversion rate.\n        by_domain: Breakdown of metrics by domain.\n        recommendations: Suggested actions for improvement.\n    \"\"\"\n\n    period: Literal[\"daily\", \"weekly\"] = Field(..., description=\"Reporting period\")\n    quality_time_score: float = Field(..., ge=0, description=\"Quality Time Score\")\n    raw_time_hours: float = Field(..., ge=0, description=\"Total raw time in hours\")\n    goal_alignment_percent: float = Field(\n        ...,\n        ge=0,\n        le=100,\n        description=\"Goal alignment percentage\",\n    )\n    distraction_percent: float = Field(\n        ...,\n        ge=0,\n        le=100,\n        description=\"Distraction percentage\",\n    )\n    actionability_score: float = Field(\n        default=0.0,\n        ge=0,\n        description=\"Actionability score\",\n    )\n    by_domain: list[DomainMetrics] = Field(\n        default_factory=list,\n        description=\"Per-domain metrics\",\n    )\n    recommendations: list[Recommendation] = Field(\n        default_factory=list,\n        description=\"Improvement recommendations\",\n    )\n</code></pre>"},{"location":"api/scoring/","title":"Scoring API","text":"<p>Composite scoring functions and report generation. These functions combine the core metrics into higher-level outputs.</p>"},{"location":"api/scoring/#generate_metrics_report","title":"generate_metrics_report","text":""},{"location":"api/scoring/#meaningful_metrics.scoring.generate_metrics_report","title":"meaningful_metrics.scoring.generate_metrics_report","text":"<pre><code>generate_metrics_report(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    goals: list[Goal],\n    actions: ActionLog | None = None,\n    period: Literal[\"daily\", \"weekly\"] = \"daily\",\n) -&gt; MetricsReport\n</code></pre> <p>Generate a complete metrics report.</p> <p>Combines all metrics into a comprehensive report with domain breakdowns and actionable recommendations.</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>priorities</code> <code>list[DomainPriority]</code> <p>List of domain priorities with optional caps.</p> required <code>goals</code> <code>list[Goal]</code> <p>List of user's goals with linked domains.</p> required <code>actions</code> <code>ActionLog | None</code> <p>Optional action log for actionability calculation.</p> <code>None</code> <code>period</code> <code>Literal['daily', 'weekly']</code> <p>Reporting period (\"daily\" or \"weekly\").</p> <code>'daily'</code> <p>Returns:</p> Type Description <code>MetricsReport</code> <p>Complete MetricsReport with all metrics and recommendations.</p> Example <p>from meaningful_metrics.schemas import TimeEntry, DomainPriority, Goal entries = [TimeEntry(domain=\"learning\", hours=2.0)] priorities = [DomainPriority(domain=\"learning\", priority=1.0)] goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])] report = generate_metrics_report(entries, priorities, goals) report.quality_time_score 2.0 report.goal_alignment_percent 100.0</p> Source code in <code>src/meaningful_metrics/scoring.py</code> <pre><code>def generate_metrics_report(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    goals: list[Goal],\n    actions: ActionLog | None = None,\n    period: Literal[\"daily\", \"weekly\"] = \"daily\",\n) -&gt; MetricsReport:\n    \"\"\"Generate a complete metrics report.\n\n    Combines all metrics into a comprehensive report with domain\n    breakdowns and actionable recommendations.\n\n    Args:\n        time_entries: List of time spent per domain.\n        priorities: List of domain priorities with optional caps.\n        goals: List of user's goals with linked domains.\n        actions: Optional action log for actionability calculation.\n        period: Reporting period (\"daily\" or \"weekly\").\n\n    Returns:\n        Complete MetricsReport with all metrics and recommendations.\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import TimeEntry, DomainPriority, Goal\n        &gt;&gt;&gt; entries = [TimeEntry(domain=\"learning\", hours=2.0)]\n        &gt;&gt;&gt; priorities = [DomainPriority(domain=\"learning\", priority=1.0)]\n        &gt;&gt;&gt; goals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])]\n        &gt;&gt;&gt; report = generate_metrics_report(entries, priorities, goals)\n        &gt;&gt;&gt; report.quality_time_score\n        2.0\n        &gt;&gt;&gt; report.goal_alignment_percent\n        100.0\n    \"\"\"\n    # Calculate core metrics\n    qts = calculate_quality_time_score(time_entries, priorities)\n    goal_alignment = calculate_goal_alignment(time_entries, goals)\n    distraction = calculate_distraction_ratio(time_entries, goals)\n\n    # Calculate actionability if action log provided\n    actionability = 0.0\n    if actions is not None:\n        actionability = calculate_actionability_score_from_log(actions)\n\n    # Calculate domain breakdowns\n    domain_metrics = calculate_domain_contributions(time_entries, priorities)\n\n    # Generate recommendations\n    recommendations = generate_recommendations(\n        time_entries, priorities, goals, goal_alignment\n    )\n\n    # Calculate raw time\n    raw_time = sum(entry.hours for entry in time_entries)\n\n    return MetricsReport(\n        period=period,\n        quality_time_score=qts,\n        raw_time_hours=raw_time,\n        goal_alignment_percent=goal_alignment,\n        distraction_percent=distraction,\n        actionability_score=actionability,\n        by_domain=domain_metrics,\n        recommendations=recommendations,\n    )\n</code></pre>"},{"location":"api/scoring/#calculate_domain_contributions","title":"calculate_domain_contributions","text":""},{"location":"api/scoring/#meaningful_metrics.scoring.calculate_domain_contributions","title":"meaningful_metrics.scoring.calculate_domain_contributions","text":"<pre><code>calculate_domain_contributions(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    *,\n    default_priority: float = 0.5\n) -&gt; list[DomainMetrics]\n</code></pre> <p>Calculate per-domain contributions to the Quality Time Score.</p> <p>Breaks down the QTS calculation to show how each domain contributes, including the effect of diminishing returns caps.</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>priorities</code> <code>list[DomainPriority]</code> <p>List of domain priorities with optional caps.</p> required <code>default_priority</code> <code>float</code> <p>Priority for domains without explicit priority.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>list[DomainMetrics]</code> <p>List of DomainMetrics with contribution details.</p> Example <p>from meaningful_metrics.schemas import TimeEntry, DomainPriority entries = [TimeEntry(domain=\"learning\", hours=3.0)] priorities = [ ...     DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=2.0) ... ] metrics = calculate_domain_contributions(entries, priorities) metrics[0].effective_time  # Capped at 2.0 2.0 metrics[0].contribution  # 2.0 * 1.0 2.0</p> Source code in <code>src/meaningful_metrics/scoring.py</code> <pre><code>def calculate_domain_contributions(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    *,\n    default_priority: float = 0.5,\n) -&gt; list[DomainMetrics]:\n    \"\"\"Calculate per-domain contributions to the Quality Time Score.\n\n    Breaks down the QTS calculation to show how each domain contributes,\n    including the effect of diminishing returns caps.\n\n    Args:\n        time_entries: List of time spent per domain.\n        priorities: List of domain priorities with optional caps.\n        default_priority: Priority for domains without explicit priority.\n\n    Returns:\n        List of DomainMetrics with contribution details.\n\n    Example:\n        &gt;&gt;&gt; from meaningful_metrics.schemas import TimeEntry, DomainPriority\n        &gt;&gt;&gt; entries = [TimeEntry(domain=\"learning\", hours=3.0)]\n        &gt;&gt;&gt; priorities = [\n        ...     DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=2.0)\n        ... ]\n        &gt;&gt;&gt; metrics = calculate_domain_contributions(entries, priorities)\n        &gt;&gt;&gt; metrics[0].effective_time  # Capped at 2.0\n        2.0\n        &gt;&gt;&gt; metrics[0].contribution  # 2.0 * 1.0\n        2.0\n    \"\"\"\n    priority_map: dict[str, DomainPriority] = {p.domain: p for p in priorities}\n    domain_metrics: list[DomainMetrics] = []\n\n    for entry in time_entries:\n        priority_config = priority_map.get(entry.domain)\n\n        if priority_config:\n            priority = priority_config.priority\n            cap = priority_config.max_daily_hours\n        else:\n            priority = default_priority\n            cap = None\n\n        effective_time = entry.hours if cap is None else min(entry.hours, cap)\n        contribution = effective_time * priority\n\n        domain_metrics.append(\n            DomainMetrics(\n                domain=entry.domain,\n                time_spent=entry.hours,\n                effective_time=effective_time,\n                priority=priority,\n                contribution=contribution,\n            )\n        )\n\n    return domain_metrics\n</code></pre>"},{"location":"api/scoring/#generate_recommendations","title":"generate_recommendations","text":""},{"location":"api/scoring/#meaningful_metrics.scoring.generate_recommendations","title":"meaningful_metrics.scoring.generate_recommendations","text":"<pre><code>generate_recommendations(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    goals: list[Goal],\n    goal_alignment: float,\n) -&gt; list[Recommendation]\n</code></pre> <p>Generate actionable recommendations based on metrics analysis.</p> <p>Analyzes time allocation against goals and priorities to suggest improvements that would increase wellbeing metrics.</p> <p>Parameters:</p> Name Type Description Default <code>time_entries</code> <code>list[TimeEntry]</code> <p>List of time spent per domain.</p> required <code>priorities</code> <code>list[DomainPriority]</code> <p>List of domain priorities.</p> required <code>goals</code> <code>list[Goal]</code> <p>List of user's goals.</p> required <code>goal_alignment</code> <code>float</code> <p>Current goal alignment percentage.</p> required <p>Returns:</p> Type Description <code>list[Recommendation]</code> <p>List of Recommendation objects with improvement suggestions.</p> Example Source code in <code>src/meaningful_metrics/scoring.py</code> <pre><code>def generate_recommendations(\n    time_entries: list[TimeEntry],\n    priorities: list[DomainPriority],\n    goals: list[Goal],\n    goal_alignment: float,\n) -&gt; list[Recommendation]:\n    \"\"\"Generate actionable recommendations based on metrics analysis.\n\n    Analyzes time allocation against goals and priorities to suggest\n    improvements that would increase wellbeing metrics.\n\n    Args:\n        time_entries: List of time spent per domain.\n        priorities: List of domain priorities.\n        goals: List of user's goals.\n        goal_alignment: Current goal alignment percentage.\n\n    Returns:\n        List of Recommendation objects with improvement suggestions.\n\n    Example:\n        &gt;&gt;&gt; # When goal alignment is low\n        &gt;&gt;&gt; recs = generate_recommendations(entries, priorities, goals, 15.0)\n        &gt;&gt;&gt; any(r.type == \"increase\" for r in recs)\n        True\n    \"\"\"\n    recommendations: list[Recommendation] = []\n    priority_map: dict[str, DomainPriority] = {p.domain: p for p in priorities}\n\n    # Collect goal domains\n    goal_domains: set[str] = set()\n    for goal in goals:\n        goal_domains.update(goal.domains)\n\n    # Analyze each time entry\n    time_by_domain: dict[str, float] = {e.domain: e.hours for e in time_entries}\n\n    # Low goal alignment recommendation\n    if goal_alignment &lt; 30.0 and goal_domains:\n        recommendations.append(\n            Recommendation(\n                type=\"increase\",\n                domain=next(iter(goal_domains)),\n                message=f\"Goal alignment is only {goal_alignment:.0f}%. Consider spending more time on goal-related activities.\",\n                priority=\"high\",\n            )\n        )\n\n    # Check for domains exceeding caps\n    for entry in time_entries:\n        priority_config = priority_map.get(entry.domain)\n        if (\n            priority_config\n            and priority_config.max_daily_hours\n            and entry.hours &gt; priority_config.max_daily_hours\n        ):\n            excess = entry.hours - priority_config.max_daily_hours\n            recommendations.append(\n                Recommendation(\n                    type=\"decrease\",\n                    domain=entry.domain,\n                    message=f\"You spent {excess:.1f}h over your cap for {entry.domain}. Consider reallocating to higher-priority activities.\",\n                    priority=\"medium\",\n                )\n            )\n\n    # Check for low-priority domains taking significant time\n    for entry in time_entries:\n        priority_config = priority_map.get(entry.domain)\n        priority = priority_config.priority if priority_config else 0.5\n\n        if priority &lt; 0.3 and entry.hours &gt; 2.0:\n            recommendations.append(\n                Recommendation(\n                    type=\"decrease\",\n                    domain=entry.domain,\n                    message=f\"Spent {entry.hours:.1f}h on low-priority domain '{entry.domain}'. Consider reducing this time.\",\n                    priority=\"medium\",\n                )\n            )\n\n    # Check for under-utilized goal domains\n    for goal in goals:\n        if goal.target_hours_per_week:\n            goal_time = sum(time_by_domain.get(d, 0.0) for d in goal.domains)\n            if goal_time &lt; goal.target_hours_per_week * 0.5:\n                recommendations.append(\n                    Recommendation(\n                        type=\"increase\",\n                        domain=goal.domains[0] if goal.domains else \"goal_activities\",\n                        message=f\"Progress on '{goal.name}' is behind target. Spent {goal_time:.1f}h vs {goal.target_hours_per_week:.0f}h target.\",\n                        priority=\"high\",\n                    )\n                )\n\n    # Positive reinforcement for good alignment\n    if goal_alignment &gt;= 50.0:\n        recommendations.append(\n            Recommendation(\n                type=\"maintain\",\n                domain=\"overall\",\n                message=f\"Great job! Goal alignment is {goal_alignment:.0f}%. Keep up the good work.\",\n                priority=\"low\",\n            )\n        )\n\n    return recommendations\n</code></pre>"},{"location":"api/scoring/#meaningful_metrics.scoring.generate_recommendations--when-goal-alignment-is-low","title":"When goal alignment is low","text":"<p>recs = generate_recommendations(entries, priorities, goals, 15.0) any(r.type == \"increase\" for r in recs) True</p>"},{"location":"case-studies/chatgpt-goal-alignment/","title":"Case Study: ChatGPT Scores 76% on Goal Alignment","text":"<p>Framework: Meaningful Metrics v0.1.0 Product Evaluated: ChatGPT (OpenAI) Evaluation Type: Cross-segment user population analysis Date: February 2024</p>"},{"location":"case-studies/chatgpt-goal-alignment/#summary","title":"Summary","text":"<p>Using the Meaningful Metrics framework, we evaluate ChatGPT as an AI product against four core dimensions: Goal Alignment, Quality Time Score, Distraction Ratio, and Actionability Score. Data is constructed from published research on ChatGPT usage patterns across three distinct user segments.</p> <p>Headline: ChatGPT scores 76% on Goal Alignment across the general user population.</p> <p>This is a stronger result than engagement-only metrics would suggest \u2014 but the per-segment breakdown reveals a critical insight: the 24% of usage time that doesn't advance user goals is concentrated in the largest user segment (Casual Explorers, 38% of users), and would be completely invisible to standard product analytics.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#context","title":"Context","text":""},{"location":"case-studies/chatgpt-goal-alignment/#why-evaluate-chatgpt","title":"Why Evaluate ChatGPT?","text":"<p>ChatGPT is the most widely adopted AI assistant product, with over 100 million weekly active users as of late 2023 (OpenAI, 2023). It represents the archetypal general-purpose AI assistant \u2014 making it an ideal first subject for demonstrating how the Meaningful Metrics framework surfaces insights that engagement metrics cannot.</p> <p>Standard ChatGPT product metrics include: daily active users, session duration, messages per session, and retention rates. None of these metrics answer the question that matters most: Is users' time with ChatGPT actually advancing their goals?</p>"},{"location":"case-studies/chatgpt-goal-alignment/#what-this-evaluation-is-and-isnt","title":"What This Evaluation Is (and Isn't)","text":"<p>This is a population-level framework demonstration, not a live product audit. Data points are derived from published research on ChatGPT usage patterns \u2014 not from OpenAI's internal analytics. The purpose is to show how Meaningful Metrics would be applied and what insights it would surface.</p> <p>This evaluation is not a criticism of ChatGPT. A 76% Goal Alignment score is genuinely strong for a general-purpose tool. The more important finding is what the framework surfaces that standard metrics hide.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#methodology","title":"Methodology","text":""},{"location":"case-studies/chatgpt-goal-alignment/#user-segments","title":"User Segments","text":"<p>We identified three primary ChatGPT user segments based on Pew Research Center (2023) usage pattern research:</p> Segment Population Share Primary Use Case Knowledge Workers 34% Professional productivity tasks Students 28% Learning and academic work Casual Explorers 38% Exploration, entertainment, curiosity"},{"location":"case-studies/chatgpt-goal-alignment/#data-sources","title":"Data Sources","text":"<p>Usage time allocation by segment: - MIT Sloan Management Review (2023): Knowledge worker AI usage patterns   (drafting 42%, summarizing 31%, brainstorming 18%, off-task 9%) - Stanford VPTL (2023): Student AI usage patterns   (essay help 58%, concept explanation 42%, problem solving 31%) - Pew Research Center (2023): Demographic breakdown and casual usage patterns - Bastani et al. (2023), NBER Working Paper: AI tutoring impact on learning outcomes;   distinguishes interactive tutoring (high retention) from passive answer-fetching (low retention)</p> <p>Actionability data: - Stanford HAI (2023) session goal achievement rates (~67% for knowledge workers) - Bastani et al. (2023) knowledge retention rates (~35% in student segment)</p>"},{"location":"case-studies/chatgpt-goal-alignment/#metrics-configuration","title":"Metrics Configuration","text":"<p>Goals and domain priorities were defined to reflect each segment's declared objectives \u2014 not inferred from behavioral data. This is intentional: Meaningful Metrics starts from explicit human intentions.</p> <p>The full evaluation script is at: <code>results/case-studies/run_chatgpt_study.py</code></p>"},{"location":"case-studies/chatgpt-goal-alignment/#findings","title":"Findings","text":""},{"location":"case-studies/chatgpt-goal-alignment/#knowledge-worker-segment-34-of-users","title":"Knowledge Worker Segment (34% of users)","text":"<pre><code>Quality Time Score:   2.54\nGoal Alignment:       92.3%\nDistraction Ratio:     7.7%\nActionability Score:  0.495\n</code></pre> <p>Interpretation: This is the highest-performing segment by a wide margin. Knowledge workers come to ChatGPT with concrete tasks and mostly execute them. The 92.3% Goal Alignment reflects the high-intent, task-specific nature of professional use.</p> <p>The 0.495 Actionability Score (out of max 1.0+ theoretical) indicates nearly half of all interactions produce a bookmarked, shared, or directly applied output \u2014 well above passive consumption.</p> <p>Notable pattern: Drafting and task completion together account for ~70% of QTS contributions. These are the use cases where ChatGPT delivers clearest value.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#student-segment-28-of-users","title":"Student Segment (28% of users)","text":"<pre><code>Quality Time Score:   3.24\nGoal Alignment:       76.5%\nDistraction Ratio:    23.5%\nActionability Score:  0.570\n</code></pre> <p>Interpretation: Students show strong goal alignment with a surprising finding: the highest QTS of any segment. This reflects prioritization \u2014 learning and problem-solving activities carry high priority weights, boosting quality score even when raw hours are similar across segments.</p> <p>The critical finding from Bastani et al. (2023) appears in the domain breakdown: <code>passive_answer_fetching</code> (0.70 hours/week) carries a 0.15 priority weight and contributes only 0.10 QTS. This domain represents the \"shortcut\" use pattern that produces high engagement metrics but poor learning outcomes.</p> <p>An engagement-optimized product would celebrate these sessions (they're long, they generate tokens, the user returns). A goal-alignment-optimized product would recognize that these sessions fail the student's actual objective (building genuine understanding) and would design prompting strategies to redirect toward interactive tutoring.</p> <p>Key recommendation surfaced: Students are spending time over the <code>drafting</code> domain cap \u2014 suggesting ChatGPT is being used for more essay generation than is healthy for learning. The framework surfaces this; standard engagement metrics would not.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#casual-explorer-segment-38-of-users","title":"Casual Explorer Segment (38% of users)","text":"<pre><code>Quality Time Score:   1.32\nGoal Alignment:       60.0%\nDistraction Ratio:    40.0%\nActionability Score:  0.225\n</code></pre> <p>Interpretation: This is the most complex segment to evaluate. \"Casual exploration\" is not inherently bad \u2014 curiosity, play, and entertainment are legitimate human needs. But this segment accounts for 38% of users and has the lowest goal alignment, suggesting significant time is spent in ways users themselves would not identify as purposeful.</p> <p>The 0.225 Actionability Score is the lowest of any segment \u2014 casual interactions rarely produce saved, shared, or applied outputs. Contrast with the knowledge worker segment's 0.495. This doesn't mean casual use is wasteful, but it does signal that ChatGPT has not found strong product-market fit for casual users seeking meaningful outcomes.</p> <p>The domain breakdown shows <code>off_task_exploration</code> (1.5 hours/week, priority 0.15) as a major time sink. This is precisely the \"dopamine scroll\" equivalent for AI assistants \u2014 engaging, low-value time that an engagement-optimized product would silently reward.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#population-weighted-aggregate","title":"Population-Weighted Aggregate","text":"<p>Weighting each segment by its share of the user population:</p> <pre><code>Quality Time Score:   2.27\nGoal Alignment:       76.0%\nDistraction Ratio:    24.4%\nActionability Score:  0.413\n</code></pre>"},{"location":"case-studies/chatgpt-goal-alignment/#interpretation","title":"Interpretation","text":"<p>76% Goal Alignment is genuinely strong. For a general-purpose tool used across radically different contexts, sustaining three-quarters of usage time on user-declared objectives is meaningful. ChatGPT is not TikTok.</p> <p>The 24% distraction gap reveals an opportunity. Almost a quarter of all ChatGPT usage time does not advance the user's stated goals. At the scale of 100 million weekly users, this represents an enormous aggregate opportunity cost.</p> <p>Actionability at 0.413 suggests moderate output quality. Fewer than half of all interactions produce an artifact the user saves, shares, or acts on. This is consistent with a product that is useful but not yet reliably purposeful.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#recommendations","title":"Recommendations","text":"<p>Based on the Meaningful Metrics analysis, three concrete product improvements would move the needle on Goal Alignment:</p>"},{"location":"case-studies/chatgpt-goal-alignment/#1-session-intention-prompts","title":"1. Session Intention Prompts","text":"<p>For: Casual Explorer segment Mechanism: At session start, prompt the user: \"What do you want to accomplish today?\" (optional, never mandatory). Map the response to a goal domain. Expected impact: Even a 15% shift of <code>off_task_exploration</code> time to <code>personal_advice</code> or <code>research_synthesis</code> would raise aggregate goal alignment by ~2-3 points.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#2-learning-mode-with-recall-prompts","title":"2. Learning Mode with Recall Prompts","text":"<p>For: Student segment Mechanism: When ChatGPT detects study-related context, offer \"Learning Mode\" that witholds direct answers, instead prompting the student to reason through the problem and offering hints. At session end, prompt recall: \"Without looking, what's the key thing you learned?\" Expected impact: Directly targets the <code>passive_answer_fetching</code> problem surfaced by Bastani et al. (2023). Would improve Actionability Score by shifting from consumption to retention.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#3-action-capture-at-session-end","title":"3. Action Capture at Session End","text":"<p>For: All segments Mechanism: After each conversation, offer one-click options: \"Save this,\" \"Share with someone,\" \"Add to my notes,\" \"I'll apply this to [task].\" Expected impact: Would directly increase Actionability Score from 0.413 toward 0.5+, and creates data for the user's own goal tracking.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#limitations","title":"Limitations","text":"<ol> <li> <p>No ground truth data. This evaluation uses research-derived approximations,    not OpenAI's internal telemetry. The segment proportions and time allocations    are estimates based on survey data.</p> </li> <li> <p>Goal alignment requires declared goals. Users who have not explicitly    declared goals cannot have goal alignment measured. The framework requires    a product-level mechanism for capturing user intentions.</p> </li> <li> <p>Domain taxonomy is an approximation. Real ChatGPT usage does not arrive    pre-labeled by domain. Applying this framework in production would require    an intent classification model or explicit user tagging.</p> </li> <li> <p>Normative assumptions. The priority weights assigned to domains    (e.g., \"passive_answer_fetching\" priority 0.15 for students) encode    value judgments. Different researchers might assign different weights.    This is a feature, not a bug \u2014 it forces explicit articulation of values    that engagement metrics leave implicit.</p> </li> </ol>"},{"location":"case-studies/chatgpt-goal-alignment/#conclusion","title":"Conclusion","text":"<p>The Meaningful Metrics framework reveals what engagement metrics hide: ChatGPT's strong overall Goal Alignment (76%) coexists with a meaningful gap in the largest user segment (Casual Explorers, 38% of users, 40% distraction ratio) and a structural problem in the Student segment (passive answer-fetching undermining the learning goal).</p> <p>This is the core argument for measurement frameworks grounded in human intentions: a product can have excellent engagement metrics and still fail to serve users well. The framework makes that failure visible, measurable, and actionable.</p>"},{"location":"case-studies/chatgpt-goal-alignment/#reproduce-this-analysis","title":"Reproduce This Analysis","text":"<pre><code># Install the package\npip install meaningful-metrics\n\n# Clone the repo\ngit clone https://github.com/jstilb/meaningful_metrics.git\ncd meaningful_metrics\n\n# Run the evaluation\npython results/case-studies/run_chatgpt_study.py\n</code></pre>"},{"location":"case-studies/chatgpt-goal-alignment/#references","title":"References","text":"<ul> <li>Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakc\u0131, \u00d6., &amp; Mariman, R. (2023).   Generative AI Can Harm Learning. NBER Working Paper.</li> <li>Pew Research Center. (2023). AI in Everyday Life: Patterns of Use Across   Demographics.</li> <li>MIT Sloan Management Review. (2023). Generative AI at Work: Early Evidence   from Professionals.</li> <li>Stanford Human-Centered AI. (2023). AI Assistant Interaction Quality: A   Multi-Study Assessment.</li> <li>Stanford VPTL. (2023). Student Use of AI Writing Tools: Patterns, Outcomes,   and Guidance.</li> <li>OpenAI. (2023). ChatGPT: 100 Million Weekly Active Users. Company blog.</li> </ul>"},{"location":"concepts/actionability/","title":"Actionability Score","text":""},{"location":"concepts/actionability/#definition","title":"Definition","text":"<p>The Actionability Score measures how effectively consumed information translates into meaningful action.</p>"},{"location":"concepts/actionability/#formula","title":"Formula","text":"<pre><code>Actionability = (Items_bookmarked + Items_shared + Items_applied) / Items_consumed\n</code></pre> <p>Or in simplified form: <pre><code>Actionability = Items_acted_on / Items_consumed\n</code></pre></p>"},{"location":"concepts/actionability/#purpose","title":"Purpose","text":""},{"location":"concepts/actionability/#the-information-overload-problem","title":"The Information Overload Problem","text":"<p>We consume vast amounts of content: - Articles read - Videos watched - Podcasts heard - Social posts scrolled</p> <p>But how much of this drives actual change?</p>"},{"location":"concepts/actionability/#why-actionability-matters","title":"Why Actionability Matters","text":"<p>Information without action is entertainment, not learning.</p> <p>A high Actionability Score indicates: - Consuming relevant, useful content - Having systems to capture and apply insights - Quality over quantity consumption</p>"},{"location":"concepts/actionability/#action-categories","title":"Action Categories","text":""},{"location":"concepts/actionability/#bookmarked","title":"Bookmarked","text":"<p>Content saved for future reference: - Read-later lists - Saved articles - Pinned posts</p> <p>Weight: 0.3 (weakest action)</p>"},{"location":"concepts/actionability/#shared","title":"Shared","text":"<p>Content shared with others: - Forwarded articles - Social shares - Recommendations</p> <p>Weight: 0.5 (moderate action)</p>"},{"location":"concepts/actionability/#applied","title":"Applied","text":"<p>Content that led to behavior change: - Notes taken and reviewed - Concepts implemented - Skills practiced</p> <p>Weight: 1.0 (strongest action)</p>"},{"location":"concepts/actionability/#weighted-formula","title":"Weighted Formula","text":"<pre><code>def calculate_actionability(\n    consumed: int,\n    bookmarked: int,\n    shared: int,\n    applied: int,\n) -&gt; float:\n    \"\"\"Calculate weighted actionability score.\n\n    Args:\n        consumed: Total items consumed.\n        bookmarked: Items saved for later.\n        shared: Items shared with others.\n        applied: Items that led to action.\n\n    Returns:\n        Actionability score (0.0 to 1.0+).\n    \"\"\"\n    if consumed == 0:\n        return 0.0\n\n    weighted_actions = (\n        bookmarked * 0.3 +\n        shared * 0.5 +\n        applied * 1.0\n    )\n\n    return weighted_actions / consumed\n</code></pre>"},{"location":"concepts/actionability/#example","title":"Example","text":""},{"location":"concepts/actionability/#weekly-consumption","title":"Weekly Consumption","text":"<ul> <li>Articles read: 50</li> <li>Videos watched: 20</li> <li>Podcasts: 5</li> <li>Total consumed: 75</li> </ul>"},{"location":"concepts/actionability/#actions-taken","title":"Actions Taken","text":"<ul> <li>Articles bookmarked: 10</li> <li>Articles shared: 5</li> <li>Concepts applied: 3</li> </ul>"},{"location":"concepts/actionability/#calculation","title":"Calculation","text":"<pre><code>Weighted actions = (10 * 0.3) + (5 * 0.5) + (3 * 1.0)\n                 = 3.0 + 2.5 + 3.0\n                 = 8.5\n\nActionability = 8.5 / 75 = 0.113 (11.3%)\n</code></pre>"},{"location":"concepts/actionability/#interpretation","title":"Interpretation","text":"<p>Only 11.3% of consumed content led to any form of action. This suggests: - Possible content overload - Need for better capture systems - Opportunity to be more selective</p>"},{"location":"concepts/actionability/#improving-actionability","title":"Improving Actionability","text":""},{"location":"concepts/actionability/#strategies","title":"Strategies","text":"<ol> <li>Consume less, act more: Quality over quantity</li> <li>Implement capture systems: Notes, highlights, reviews</li> <li>Regular review cycles: Weekly review of bookmarks</li> <li>Action-first filtering: \"Will I do anything with this?\"</li> </ol>"},{"location":"concepts/actionability/#tools-that-help","title":"Tools That Help","text":"<ul> <li>Readwise (capture and resurface)</li> <li>Anki (spaced repetition for learning)</li> <li>Action-oriented note systems (Zettelkasten)</li> </ul>"},{"location":"concepts/actionability/#benchmarks","title":"Benchmarks","text":"Score Interpretation &lt; 5% Consumption mode (passive) 5-15% Typical range 15-30% Active learner 30%+ Highly selective, action-oriented"},{"location":"concepts/actionability/#limitations","title":"Limitations","text":""},{"location":"concepts/actionability/#tracking-challenges","title":"Tracking Challenges","text":"<p>\"Applied\" is hard to track automatically: - Requires user input - Subjective judgment - Delayed effects</p>"},{"location":"concepts/actionability/#not-all-content-needs-action","title":"Not All Content Needs Action","text":"<p>Some content is legitimately for: - Entertainment - Rest - Idle curiosity</p> <p>Consider domain-specific actionability rather than global.</p>"},{"location":"concepts/actionability/#related-metrics","title":"Related Metrics","text":"<ul> <li>Quality Time Score: Weights time by priority</li> <li>Goal Alignment: Time on goal-related activities</li> <li>Learning Velocity: Rate of skill acquisition</li> </ul>"},{"location":"concepts/goal-alignment/","title":"Goal Alignment Metric","text":""},{"location":"concepts/goal-alignment/#definition","title":"Definition","text":"<p>Goal Alignment measures the percentage of time spent on activities that directly support the user's stated goals.</p>"},{"location":"concepts/goal-alignment/#formula","title":"Formula","text":"<pre><code>Goal Alignment = (sum(Time_goal_domains) / Total_time) * 100%\n</code></pre> <p>Where: - Time_goal_domains = Time spent in domains linked to active goals - Total_time = Total tracked time</p>"},{"location":"concepts/goal-alignment/#purpose","title":"Purpose","text":""},{"location":"concepts/goal-alignment/#why-track-goal-alignment","title":"Why Track Goal Alignment?","text":"<p>Users often have intentions that don't match their behavior: - \"I want to learn to code\" (goal) - But spend 3 hours on social media (behavior)</p> <p>Goal Alignment quantifies this gap.</p>"},{"location":"concepts/goal-alignment/#relationship-to-qts","title":"Relationship to QTS","text":"<p>While QTS weights all time by priority, Goal Alignment focuses specifically on: - Time that advances stated objectives - Binary classification: goal-related or not</p>"},{"location":"concepts/goal-alignment/#example","title":"Example","text":""},{"location":"concepts/goal-alignment/#users-goals","title":"User's Goals","text":"<ol> <li>Learn Spanish - domains: [\"language_learning\", \"spanish_content\"]</li> <li>Get fit - domains: [\"exercise\", \"fitness_content\", \"nutrition\"]</li> </ol>"},{"location":"concepts/goal-alignment/#time-spent-weekly","title":"Time Spent (Weekly)","text":"Domain Hours language_learning 5 social_media 10 work 30 exercise 3 entertainment 8"},{"location":"concepts/goal-alignment/#calculation","title":"Calculation","text":"<p>Goal-related domains: language_learning (5h) + exercise (3h) = 8 hours Total tracked: 56 hours</p> <pre><code>Goal Alignment = 8 / 56 * 100% = 14.3%\n</code></pre>"},{"location":"concepts/goal-alignment/#interpretation","title":"Interpretation","text":"<p>Only 14.3% of this user's time directly supports their goals. This provides clear feedback for behavior change.</p>"},{"location":"concepts/goal-alignment/#implementation","title":"Implementation","text":"<pre><code>def calculate_goal_alignment(\n    time_entries: list[TimeEntry],\n    goals: list[Goal],\n) -&gt; float:\n    \"\"\"Calculate goal alignment percentage.\n\n    Args:\n        time_entries: Time spent per domain.\n        goals: User's defined goals with linked domains.\n\n    Returns:\n        Percentage of time spent on goal-related activities.\n    \"\"\"\n    goal_domains = set()\n    for goal in goals:\n        goal_domains.update(goal.domains)\n\n    total_time = sum(e.hours for e in time_entries)\n    goal_time = sum(\n        e.hours for e in time_entries\n        if e.domain in goal_domains\n    )\n\n    if total_time == 0:\n        return 0.0\n\n    return (goal_time / total_time) * 100\n</code></pre>"},{"location":"concepts/goal-alignment/#considerations","title":"Considerations","text":""},{"location":"concepts/goal-alignment/#domain-mapping","title":"Domain Mapping","text":"<p>Goals should map to trackable domains: - Goal: \"Learn to code\" -&gt; Domains: [\"coding\", \"tutorials\", \"documentation\"] - Goal: \"Stay informed\" -&gt; Domains: [\"news\", \"newsletters\", \"podcasts\"]</p>"},{"location":"concepts/goal-alignment/#exclusions","title":"Exclusions","text":"<p>Some domains shouldn't count as \"distraction\": - Sleep - Meals - Necessary errands</p> <p>Consider a \"neutral\" category that doesn't affect Goal Alignment.</p>"},{"location":"concepts/goal-alignment/#target-ranges","title":"Target Ranges","text":"<p>Suggested goal alignment targets: - 10-20%: Low alignment (common starting point) - 30-40%: Moderate alignment - 50%+: High alignment (aspirational for most)</p> <p>Note: 100% alignment is not the goal. Rest, recovery, and leisure are important.</p>"},{"location":"concepts/goal-alignment/#related-metrics","title":"Related Metrics","text":"<ul> <li>Distraction Ratio: The inverse (time on non-goal activities)</li> <li>Quality Time Score: Weighted time including goals</li> <li>Progress Velocity: Rate of goal completion over time</li> </ul>"},{"location":"concepts/quality-time-score/","title":"Quality Time Score (QTS)","text":""},{"location":"concepts/quality-time-score/#definition","title":"Definition","text":"<p>The Quality Time Score is the primary metric in the Meaningful Metrics framework. It provides a weighted measure of time spent that accounts for user priorities and diminishing returns.</p>"},{"location":"concepts/quality-time-score/#formula","title":"Formula","text":"<pre><code>QTS = sum(min(Ti, Capi) * Pi)\n</code></pre> <p>Where: - Ti = Time spent in domain i (hours) - Capi = Maximum valuable hours for domain i (diminishing returns cap) - Pi = Priority score for domain i (0.0 - 1.0)</p>"},{"location":"concepts/quality-time-score/#rationale","title":"Rationale","text":""},{"location":"concepts/quality-time-score/#why-not-raw-time","title":"Why Not Raw Time?","text":"<p>Raw time treats all hours equally: - 1 hour learning = 1 hour scrolling social media - 4 hours of focused work = 4 hours of distracted browsing</p> <p>This fails to capture the quality of time spent.</p>"},{"location":"concepts/quality-time-score/#priority-weighting","title":"Priority Weighting","text":"<p>Users define what matters to them: - Learning might have priority 1.0 - Entertainment might have priority 0.5 - Mindless scrolling might have priority 0.1</p> <p>The QTS weights time accordingly.</p>"},{"location":"concepts/quality-time-score/#diminishing-returns","title":"Diminishing Returns","text":"<p>The cap prevents over-optimization: - Reading for 4 hours when 2 hours is optimal = 2 hours contribution - This encourages balanced time allocation - Prevents \"gaming\" the metric with excessive time in one area</p>"},{"location":"concepts/quality-time-score/#example-calculation","title":"Example Calculation","text":""},{"location":"concepts/quality-time-score/#scenario","title":"Scenario","text":"<p>A user's time allocation for a day: - Learning: 3 hours - Work: 5 hours - Social Media: 2 hours - News: 1 hour</p> <p>Their priorities: - Learning: P=1.0, Cap=4h - Work: P=0.8, Cap=8h - Social Media: P=0.2, Cap=1h - News: P=0.5, Cap=1h</p>"},{"location":"concepts/quality-time-score/#calculation","title":"Calculation","text":"<pre><code>QTS = min(3, 4) * 1.0    # Learning: 3 * 1.0 = 3.0\n    + min(5, 8) * 0.8    # Work: 5 * 0.8 = 4.0\n    + min(2, 1) * 0.2    # Social Media: 1 * 0.2 = 0.2 (capped!)\n    + min(1, 1) * 0.5    # News: 1 * 0.5 = 0.5\n\nQTS = 3.0 + 4.0 + 0.2 + 0.5 = 7.7\n</code></pre>"},{"location":"concepts/quality-time-score/#comparison-to-raw-time","title":"Comparison to Raw Time","text":"<p>Raw time = 3 + 5 + 2 + 1 = 11 hours</p> <p>QTS efficiency = 7.7 / 11 = 70%</p> <p>This tells the user that 70% of their time was spent meaningfully according to their own priorities.</p>"},{"location":"concepts/quality-time-score/#implementation-notes","title":"Implementation Notes","text":""},{"location":"concepts/quality-time-score/#differentiability","title":"Differentiability","text":"<p>The QTS formula uses <code>min()</code> which has a subgradient at the boundary. For smooth optimization, consider using a soft-min approximation:</p> <pre><code>def soft_min(a: float, b: float, alpha: float = 10.0) -&gt; float:\n    \"\"\"Smooth approximation of min(a, b) for differentiability.\"\"\"\n    return -1/alpha * log(exp(-alpha * a) + exp(-alpha * b))\n</code></pre>"},{"location":"concepts/quality-time-score/#edge-cases","title":"Edge Cases","text":"<ol> <li>Missing priorities: Default to 0.5 (neutral)</li> <li>No cap defined: Use infinity (no diminishing returns)</li> <li>Zero time: QTS contribution is 0</li> <li>Negative time: Raise validation error</li> </ol>"},{"location":"concepts/quality-time-score/#recommended-defaults","title":"Recommended Defaults","text":"Domain Default Priority Default Cap Learning 1.0 4 hours Deep Work 0.9 6 hours Communication 0.6 2 hours News 0.5 1 hour Entertainment 0.4 2 hours Social Media 0.2 1 hour"},{"location":"concepts/quality-time-score/#related-metrics","title":"Related Metrics","text":"<ul> <li>Goal Alignment: Measures what percentage of time goes to goal-related domains</li> <li>Distraction Ratio: Inverse - time on non-goal activities</li> <li>Actionability Score: Whether consumed content leads to action</li> </ul>"},{"location":"concepts/quality-time-score/#references","title":"References","text":"<ul> <li>Attention Economics (Herbert Simon)</li> <li>Flow State Research (Csikszentmihalyi)</li> <li>Time Blocking Methodology (Cal Newport)</li> </ul>"},{"location":"decisions/001-pydantic-for-schemas/","title":"ADR-001: Use Pydantic v2 for Data Schemas","text":""},{"location":"decisions/001-pydantic-for-schemas/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"decisions/001-pydantic-for-schemas/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"decisions/001-pydantic-for-schemas/#context","title":"Context","text":"<p>The Meaningful Metrics library needs a data validation layer that:</p> <ul> <li>Enforces type safety at runtime for user-provided inputs</li> <li>Provides clear error messages when inputs are invalid</li> <li>Supports serialization to/from JSON for API integration</li> <li>Works well with Python type checkers (mypy)</li> <li>Has minimal overhead for the computational workload</li> </ul> <p>Options considered: 1. Pydantic v2 -- Industry standard for data validation in Python 2. dataclasses + manual validation -- Lightweight but requires boilerplate 3. attrs -- Powerful but less ecosystem support than Pydantic 4. TypedDict -- Zero runtime overhead but no validation</p>"},{"location":"decisions/001-pydantic-for-schemas/#decision","title":"Decision","text":"<p>Use Pydantic v2 (<code>BaseModel</code>) for all input and output schemas.</p>"},{"location":"decisions/001-pydantic-for-schemas/#rationale","title":"Rationale","text":"<ul> <li>Runtime validation: Pydantic catches invalid inputs (negative hours, out-of-range priorities) before they reach metric calculations, preventing silent mathematical errors.</li> <li>JSON serialization: Built-in <code>.model_dump()</code> and <code>.model_validate()</code> make it trivial to serialize reports for dashboards and APIs.</li> <li>Developer experience: Pydantic v2's <code>Field()</code> descriptors serve as inline documentation, and validation errors include the field name and constraint that failed.</li> <li>Type checker compatibility: Pydantic v2's generated <code>__init__</code> signatures work with mypy in strict mode.</li> <li>Performance: Pydantic v2 uses Rust-based validation (pydantic-core), making validation overhead negligible compared to the metric calculations themselves.</li> </ul>"},{"location":"decisions/001-pydantic-for-schemas/#consequences","title":"Consequences","text":"<ul> <li>Positive: Clean separation between validation (schemas) and computation (metrics). New contributors can add fields without touching metric logic.</li> <li>Positive: JSON serialization enables integration with dashboards, APIs, and data pipelines with zero additional code.</li> <li>Negative: Adds <code>pydantic&gt;=2.0</code> as a runtime dependency. Acceptable given Pydantic's ubiquity in the Python ecosystem.</li> <li>Negative: Slight learning curve for contributors unfamiliar with Pydantic model patterns.</li> </ul>"},{"location":"decisions/002-pure-functions-for-metrics/","title":"ADR-002: Pure Functions for Metric Calculations","text":""},{"location":"decisions/002-pure-functions-for-metrics/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"decisions/002-pure-functions-for-metrics/#date","title":"Date","text":"<p>2024-12-01</p>"},{"location":"decisions/002-pure-functions-for-metrics/#context","title":"Context","text":"<p>The core metric calculations (Quality Time Score, Goal Alignment, etc.) need an implementation pattern that:</p> <ul> <li>Makes formulas auditable and testable</li> <li>Supports both standalone usage and composition into reports</li> <li>Enables future ML optimization (gradient-based training)</li> <li>Keeps the codebase simple for open-source contributors</li> </ul> <p>Options considered: 1. Pure functions -- Stateless functions taking typed inputs 2. Class-based calculators -- <code>MetricsCalculator</code> with methods and internal state 3. Strategy pattern -- Abstract base class with pluggable implementations 4. Pipeline/chain pattern -- Composable transformation steps</p>"},{"location":"decisions/002-pure-functions-for-metrics/#decision","title":"Decision","text":"<p>Implement all metrics as pure functions in <code>metrics.py</code>, composed by higher-level functions in <code>scoring.py</code>.</p>"},{"location":"decisions/002-pure-functions-for-metrics/#rationale","title":"Rationale","text":"<ul> <li>Testability: Pure functions with no side effects are trivial to unit test -- provide inputs, assert outputs. No setup/teardown, no mocking.</li> <li>Auditability: Each function's docstring contains the exact mathematical formula. Reviewers can verify correctness by reading a single function.</li> <li>Composability: <code>scoring.py</code> composes individual metrics into reports without coupling. Adding a new metric means adding one function and one line in the report generator.</li> <li>ML compatibility: Pure functions can be wrapped in differentiable frameworks. The <code>soft_min</code> utility demonstrates this -- a smooth approximation of <code>min()</code> for gradient computation.</li> <li>Simplicity: No class hierarchies, no state management, no dependency injection. A contributor can understand any metric by reading one function.</li> </ul>"},{"location":"decisions/002-pure-functions-for-metrics/#consequences","title":"Consequences","text":"<ul> <li>Positive: Each metric is independently testable with ~5 lines of test code per case.</li> <li>Positive: The <code>scoring.py</code> composition layer is a thin orchestrator, easy to extend.</li> <li>Negative: Shared computation (e.g., priority lookup) is duplicated across functions. Acceptable at current scale; can be refactored if metrics grow beyond 10+.</li> <li>Negative: No caching of intermediate results. Acceptable because the computational cost is trivial (linear in number of time entries).</li> </ul>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":""},{"location":"examples/basic_usage/#setup","title":"Setup","text":"<p>First, install the library:</p> <pre><code>pip install meaningful-metrics\n</code></pre>"},{"location":"examples/basic_usage/#example-1-calculate-quality-time-score","title":"Example 1: Calculate Quality Time Score","text":"<pre><code>from meaningful_metrics import calculate_quality_time_score\nfrom meaningful_metrics.schemas import TimeEntry, DomainPriority\n\n# Define your domain priorities\npriorities = [\n    DomainPriority(\n        domain=\"learning\",\n        priority=1.0,\n        max_daily_hours=4.0,\n    ),\n    DomainPriority(\n        domain=\"work\",\n        priority=0.8,\n        max_daily_hours=8.0,\n    ),\n    DomainPriority(\n        domain=\"social_media\",\n        priority=0.2,\n        max_daily_hours=1.0,\n    ),\n    DomainPriority(\n        domain=\"entertainment\",\n        priority=0.4,\n        max_daily_hours=2.0,\n    ),\n]\n\n# Track your daily time\ntime_entries = [\n    TimeEntry(domain=\"learning\", hours=2.5),\n    TimeEntry(domain=\"work\", hours=6.0),\n    TimeEntry(domain=\"social_media\", hours=1.5),  # Over the cap!\n    TimeEntry(domain=\"entertainment\", hours=1.0),\n]\n\n# Calculate QTS\nqts = calculate_quality_time_score(time_entries, priorities)\n\nprint(f\"Raw time: {sum(e['hours'] for e in time_entries)} hours\")\nprint(f\"Quality Time Score: {qts:.2f}\")\n\n# Output:\n# Raw time: 11.0 hours\n# Quality Time Score: 7.5\n# (Social media capped at 1h, so 1.5h only contributes 1*0.2=0.2)\n</code></pre>"},{"location":"examples/basic_usage/#example-2-track-goal-alignment","title":"Example 2: Track Goal Alignment","text":"<pre><code>from meaningful_metrics import calculate_goal_alignment\nfrom meaningful_metrics.schemas import TimeEntry, Goal\n\n# Define your goals\ngoals = [\n    Goal(\n        id=\"learn-spanish\",\n        name=\"Learn Spanish\",\n        domains=[\"language_learning\", \"spanish_media\"],\n        target_hours_per_week=7.0,\n    ),\n    Goal(\n        id=\"get-fit\",\n        name=\"Get Fit\",\n        domains=[\"exercise\", \"fitness_content\"],\n        target_hours_per_week=5.0,\n    ),\n]\n\n# Weekly time entries\ntime_entries = [\n    TimeEntry(domain=\"language_learning\", hours=4.0),\n    TimeEntry(domain=\"work\", hours=40.0),\n    TimeEntry(domain=\"exercise\", hours=3.0),\n    TimeEntry(domain=\"social_media\", hours=7.0),\n    TimeEntry(domain=\"entertainment\", hours=10.0),\n]\n\n# Calculate alignment\nalignment = calculate_goal_alignment(time_entries, goals)\n\nprint(f\"Goal Alignment: {alignment:.1f}%\")\n# Output: Goal Alignment: 10.9%\n# (7 hours on goals out of 64 total hours)\n</code></pre>"},{"location":"examples/basic_usage/#example-3-generate-full-report","title":"Example 3: Generate Full Report","text":"<pre><code>from meaningful_metrics import generate_metrics_report\nfrom meaningful_metrics.schemas import (\n    TimeEntry,\n    DomainPriority,\n    Goal,\n    ActionLog,\n)\n\n# Set up all inputs\npriorities = [\n    DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=4.0),\n    DomainPriority(domain=\"work\", priority=0.8, max_daily_hours=8.0),\n    DomainPriority(domain=\"social_media\", priority=0.2, max_daily_hours=1.0),\n]\n\ngoals = [\n    Goal(id=\"upskill\", name=\"Upskill\", domains=[\"learning\", \"tutorials\"]),\n]\n\ntime_entries = [\n    TimeEntry(domain=\"learning\", hours=2.0),\n    TimeEntry(domain=\"work\", hours=5.0),\n    TimeEntry(domain=\"social_media\", hours=2.0),\n]\n\nactions = ActionLog(\n    consumed=50,\n    bookmarked=10,\n    shared=3,\n    applied=2,\n)\n\n# Generate report\nreport = generate_metrics_report(\n    time_entries=time_entries,\n    priorities=priorities,\n    goals=goals,\n    actions=actions,\n    period=\"daily\",\n)\n\n# Print results\nprint(f\"Quality Time Score: {report['quality_time_score']:.2f}\")\nprint(f\"Raw Hours: {report['raw_time_hours']:.1f}\")\nprint(f\"Goal Alignment: {report['goal_alignment_percent']:.1f}%\")\nprint(f\"Actionability: {report['actionability_score']:.2%}\")\n\nprint(\"\\nDomain Breakdown:\")\nfor domain in report[\"by_domain\"]:\n    print(f\"  {domain['domain']}: {domain['contribution']:.2f} contribution\")\n\nprint(\"\\nRecommendations:\")\nfor rec in report[\"recommendations\"]:\n    print(f\"  [{rec['priority']}] {rec['message']}\")\n</code></pre>"},{"location":"examples/basic_usage/#example-4-custom-actionability-weights","title":"Example 4: Custom Actionability Weights","text":"<pre><code>from meaningful_metrics import calculate_actionability_score\nfrom meaningful_metrics.schemas import ActionWeights\n\n# Custom weights for your workflow\nweights = ActionWeights(\n    bookmarked=0.2,  # Bookmarking is less valuable to you\n    shared=0.3,      # Sharing isn't a priority\n    applied=1.5,     # Application is extra valuable\n)\n\nscore = calculate_actionability_score(\n    consumed=100,\n    bookmarked=20,\n    shared=5,\n    applied=10,\n    weights=weights,\n)\n\nprint(f\"Actionability: {score:.2%}\")\n</code></pre>"},{"location":"examples/basic_usage/#example-5-weekly-trends","title":"Example 5: Weekly Trends","text":"<pre><code>from meaningful_metrics import generate_metrics_report\nfrom meaningful_metrics.schemas import TimeEntry, DomainPriority, Goal\n\n# Define consistent priorities and goals\npriorities = [\n    DomainPriority(domain=\"learning\", priority=1.0, max_daily_hours=4.0),\n    DomainPriority(domain=\"work\", priority=0.8),\n]\n\ngoals = [Goal(id=\"learn\", name=\"Learn\", domains=[\"learning\"])]\n\n# Track multiple days\ndaily_entries = {\n    \"monday\": [\n        TimeEntry(domain=\"learning\", hours=2.0),\n        TimeEntry(domain=\"work\", hours=8.0),\n    ],\n    \"tuesday\": [\n        TimeEntry(domain=\"learning\", hours=1.0),\n        TimeEntry(domain=\"work\", hours=9.0),\n    ],\n    \"wednesday\": [\n        TimeEntry(domain=\"learning\", hours=3.0),\n        TimeEntry(domain=\"work\", hours=7.0),\n    ],\n}\n\n# Generate daily reports\nfor day, entries in daily_entries.items():\n    report = generate_metrics_report(entries, priorities, goals)\n    print(f\"{day.capitalize()}: QTS={report['quality_time_score']:.2f}, \"\n          f\"Alignment={report['goal_alignment_percent']:.0f}%\")\n\n# Output:\n# Monday: QTS=8.4, Alignment=20%\n# Tuesday: QTS=8.2, Alignment=10%\n# Wednesday: QTS=8.6, Alignment=30%\n</code></pre>"},{"location":"toolkit/","title":"Validation &amp; Governance Toolkit","text":"<p>Use these reusable assets to support consistent evaluation quality across metrics and AI playbooks. Each template is designed to pair with the contribution maturity tiers defined in the README.</p> <ul> <li><code>fairness_audit_checklist.md</code> \u2014 Structured questions for identifying and mitigating bias.</li> <li><code>participant_survey_template.md</code> \u2014 Standardized survey for measuring perceived impact and trust.</li> <li><code>decision_log_template.md</code> \u2014 Shared format for capturing review outcomes, commitments, and follow-up tasks.</li> <li><code>accessibility_debt_register.csv</code> \u2014 Tracker for outstanding accessibility issues and remediation SLAs.</li> </ul> <p>Contributors should copy these assets into their internal documentation, customize responsibly, and link back to updates in the community changelog.</p>"},{"location":"toolkit/decision_log_template/","title":"Decision Log Template","text":"Date Artifact Maturity Tier Decision Summary Owners Due Date Follow-up Evidence YYYY-MM-DD metric/eval name Idea/Draft/Field-Tested What was decided? Name + role YYYY-MM-DD Link to data, changelog entry, or remediation proof"},{"location":"toolkit/decision_log_template/#how-to-use","title":"How to Use","text":"<ol> <li>Create a new row for each review meeting or major decision.</li> <li>Provide context on why the decision was made, referencing survey data, fairness audits, or community feedback.</li> <li>Update the <code>Follow-up Evidence</code> column once actions are complete; link to issues, dashboards, or community updates.</li> <li>Archive completed decisions quarterly in the community changelog.</li> </ol>"},{"location":"toolkit/fairness_audit_checklist/","title":"Fairness Audit Checklist","text":"<p>Use this checklist during quarterly reviews or before launching new metrics/evaluations.</p>"},{"location":"toolkit/fairness_audit_checklist/#preparation","title":"Preparation","text":"<ul> <li>[ ] Confirm data sources include documentation for collection methods and known limitations.</li> <li>[ ] Verify demographic slices are representative and align with community definitions.</li> <li>[ ] Identify stakeholders who can validate the framing (community partners, subject-matter experts).</li> </ul>"},{"location":"toolkit/fairness_audit_checklist/#quantitative-review","title":"Quantitative Review","text":"<ul> <li>[ ] Compute outcome metrics across protected groups and intersectional slices.</li> <li>[ ] Highlight segments with \u226510 percentage point gaps or &lt;0.8\u00d7 parity ratios compared to the best-performing group.</li> <li>[ ] Document statistical significance tests and confidence intervals.</li> </ul>"},{"location":"toolkit/fairness_audit_checklist/#qualitative-review","title":"Qualitative Review","text":"<ul> <li>[ ] Conduct interviews or focus groups with impacted communities and summarize themes.</li> <li>[ ] Validate that accessibility accommodations were provided during research.</li> <li>[ ] Ensure translation/localization needs were met for non-dominant languages.</li> </ul>"},{"location":"toolkit/fairness_audit_checklist/#remediation-governance","title":"Remediation &amp; Governance","text":"<ul> <li>[ ] Assign owners and due dates for each identified gap using the decision log template.</li> <li>[ ] Publish a summary of findings to the community changelog within two weeks.</li> <li>[ ] Schedule a follow-up audit to confirm remediation effectiveness.</li> </ul>"},{"location":"toolkit/participant_survey_template/","title":"Participant Survey Template","text":"<p>Use this survey after pilots or evaluations to capture perceived impact and trust. Adapt language to local context and ensure accessibility accommodations are provided.</p> <ol> <li>In your own words, what task were you trying to accomplish?</li> <li>How confident did you feel while completing the task? (1 - Not confident, 5 - Very confident)</li> <li>How helpful was the product/AI system in supporting you? (1 - Not helpful, 5 - Extremely helpful)</li> <li>Did you encounter any barriers or frustrations? (Open text)</li> <li>How satisfied are you with the outcome you achieved? (1 - Very dissatisfied, 5 - Very satisfied)</li> <li>Did the experience respect your privacy and preferences? (Yes/No/Unsure with optional text)</li> <li>How inclusive and accessible did the experience feel? (1 - Not inclusive, 5 - Very inclusive)</li> <li>Would you recommend this experience to others in your community? (Yes/No/Depends + explanation)</li> <li>What should we improve next? (Open text)</li> </ol> <p>Collect responses anonymously when possible, and summarize key themes in the community changelog along with planned follow-up actions.</p>"}]}